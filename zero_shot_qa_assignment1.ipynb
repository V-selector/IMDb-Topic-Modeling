{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq4OjTS7NwK-"
   },
   "source": [
    "## Project 1\n",
    "# Zero-Shot Question Answering\n",
    "The aim of this project is to get familiar with Language Models for Zero-Shot Question Answering and possible pitfalls when it comes to mesuring LLM performance on common benchmarks.  \n",
    "The project is divided into two parts:\n",
    "1. **Encoder Models**:\n",
    "    - Here you will see how to used predefined HF / transformers classes to solve this task\n",
    "2. **Decoder Models**:\n",
    "    - Here you will see how to adapt a decoder model to solve this task and how are modern LLMs benchmarked on this task.\n",
    "\n",
    "### What is Zero-Shot Question Answering?\n",
    "Zero-shot question answering is a task where a model is given a question and a context, and the model is expected to predict the answer without any training on the context or the question. The model is expected to generalize to unseen context and questions. From practical perspective it is a situation where we want to use a model to our task without any fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIsuYUxQNwLA"
   },
   "source": [
    "### Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qt8nRNCzNwLB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: networkx in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from torch>=2.0->transformers[torch]) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from requests->transformers[torch]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sandra/Desktop/venv/lib/python3.13/site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuajMlW3NwLC"
   },
   "source": [
    "### Part 1: Dataset\n",
    "\n",
    "We will work on [MMLU dataset](https://huggingface.co/datasets/CohereForAI/Global-MMLU). Let's have a look at examples from the dataset. For each question we are given 4 answers, the correct one and the subject of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JY0tz1HoNwLC",
    "outputId": "61edfed2-b566-47de-ed89-cbb4fd013afe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandra/Desktop/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = load_dataset(\"CohereForAI/Global-MMLU\", \"en\", split=\"test\")\n",
    "\n",
    "def preprocess(sample: dict):\n",
    "    return {\n",
    "        \"options\": [\n",
    "            sample[option]\n",
    "            for option in [\"option_a\", \"option_b\", \"option_c\", \"option_d\"]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J6KQ5YgFNwLD",
    "outputId": "23203dc5-597f-4bde-c12f-f953e8af690f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Examples: 14042\n",
      "Mean length: 274.54\n",
      "Max length: 4671\n"
     ]
    }
   ],
   "source": [
    "print(f\"N Examples: {len(ds)}\")\n",
    "print(f\"Mean length: {sum(len(x['question']) for x in ds) / len(ds):4.2f}\")\n",
    "print(f\"Max length: {max(len(x['question']) for x in ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ib3z1nYmNwLD",
    "outputId": "9a67381a-71c3-41cd-b221-009ec4ebab28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "Sample subject: abstract_algebra\n",
      "Options:\n",
      " A: 0\n",
      "B: 4\n",
      "C: 2\n",
      "D: 6\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "sample_question = ds[sample_idx][\"question\"]\n",
    "sample_subject = ds[sample_idx][\"subject\"]\n",
    "options = ds[sample_idx][\"options\"]\n",
    "answer = ds[sample_idx][\"answer\"]\n",
    "\n",
    "print(\"Sample question:\", sample_question)\n",
    "print(\"Sample subject:\", sample_subject)\n",
    "print(\"Options:\\n\", \"\\n\".join([f\"{c.upper()}: {o}\" for c, o in zip(\"abcd\", options)]))\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2E1t7b9NwLE"
   },
   "source": [
    "### Part 2: Encoder Models\n",
    "\n",
    "Let's have a look how to use out of the box transformers pipeline to solve this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2bYz1dJ9NwLE",
    "outputId": "124a266a-ee51-4690-9ed4-7446228344aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "THHido-rNwLE",
    "outputId": "bd3b3e6b-8191-481e-dce9-fb81b07f3907"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model with `torch.compile` and using a `torch.mps` device is not supported. Falling back to non-compiled mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n",
       " 'labels': ['6', '4', '2', '0'],\n",
       " 'scores': [0.27904778718948364,\n",
       "  0.27592524886131287,\n",
       "  0.23753005266189575,\n",
       "  0.20749692618846893]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_classifier(\n",
    "    sample_question,\n",
    "    options,\n",
    "    hypothesis_template=\"The correct answer is: {}\",\n",
    "    multi_label=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDESO1FNwLF"
   },
   "source": [
    "#### How it works under the hood?\n",
    "\n",
    "If you go to the [source code](https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src/transformers/pipelines/zero_shot_classification.py#L49) you can see that it uses `ModelForSequenceClassification` and in [model card](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0) you can read that the model was in fact fine tuned on question answering task.  \n",
    "The base model used for fine-tuning is ModernBERT, which is a modernized version of the BERT model, making use of various advancements in the *atention* mechanism, improving both performance and efficiency.  \n",
    "If you are interested in details, we highly recommend the following [Hugging Face blogpost](https://huggingface.co/blog/modernbert).\n",
    "\n",
    "By digging deeper in [model config](https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0/blob/a51e07b524299e309dd2b88d48b0cfa2bd9ec598/config.json#L24) we can see that the only labels the model knows about are\n",
    "```\n",
    "\"id2label\": {\n",
    "    \"0\": \"entailment\",\n",
    "    \"1\": \"not_entailment\"\n",
    "  }\n",
    "```\n",
    "\n",
    "For each option the model classifies the text\n",
    "```\n",
    "{question}\n",
    "{hypothesis_template} {option}\n",
    "```\n",
    "as either entailment or not entailment. The option with the highest entailment score is the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbxpBtZSNwLF"
   },
   "source": [
    "#### Task: evaluate the model on the dataset\n",
    "Your task is to evaluate the model on the dataset and calculate some metrics (accuracy, potentially some other metrics and more granular insight - e.g. per question subject).  \n",
    "Additionally you will implement batching to improve the evaluation performance and use profiler to analyze the improvements.\n",
    "\n",
    "Note that our problem is not typical classification task because the classes (here: available answers) are different for each question.  \n",
    "The \"zero-shot-classification\" pipeline expects that the *classes* passed to it are the same for all examples in the batch.  \n",
    "To overcome this limitation we need to reimplement the pipeline.\n",
    "\n",
    "The task involves the following steps:\n",
    "\n",
    "    1. First, implement a naive function which given the dataset (or its subset) processes it row by row using the zero-shot pipeline. (1 pkt)\n",
    "    2. Implement a vectorized (batched) version of the pipeline. (4 pkt)\n",
    "    3. Write a test function comparing the results of batching with the naive version. (1 pkt)\n",
    "    4. Profile the batched version and (adaptively) choose the best batch size for processing the whole dataset. (2 pkt)\n",
    "    5. Calculate accuracy of the model and some more insight on the results. (2pkt)\n",
    "        Batching is not strictly required for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upTd20U_NwLF"
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pzNglZxvNwLF"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from textwrap import dedent\n",
    "import torch\n",
    "\n",
    "\n",
    "QUESTION_TEMPLATE = dedent(\n",
    "    \"\"\"\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "HYPOTHESIS_TEMPLATE = \"The correct answer is: {}\"\n",
    "\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IukVvAFdNwLF"
   },
   "source": [
    "#### Naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ipOF0loWNwLF"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class PipelineResult(TypedDict):\n",
    "    labels: list[list[str]] #sorted according to scores\n",
    "    scores: list[list[float]] #sorted descending\n",
    "    top_inds: list[list[int]] #for each label, its index in the input's options list\n",
    "\n",
    "\n",
    "def naive_zero_shot_classifier_pipeline(zero_shot_classifier, pipeline_input: Dataset) -> PipelineResult:\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    all_top_inds = []\n",
    "\n",
    "    for sample in tqdm(pipeline_input):\n",
    "        question = sample[\"question\"]\n",
    "        options = sample[\"options\"]\n",
    "\n",
    "        result = zero_shot_classifier(\n",
    "            question,\n",
    "            options,\n",
    "            hypothesis_template=\"The correct answer is: {}\",\n",
    "            multi_label=False,\n",
    "        )\n",
    "\n",
    "        sorted_labels = result[\"labels\"]\n",
    "        sorted_scores = result[\"scores\"]\n",
    "        top_indices = [options.index(label) for label in sorted_labels]\n",
    "\n",
    "        all_labels.append(sorted_labels)\n",
    "        all_scores.append(sorted_scores)\n",
    "        all_top_inds.append(top_indices)\n",
    "\n",
    "    return {\n",
    "        \"labels\": all_labels,\n",
    "        \"scores\": all_scores,\n",
    "        \"top_inds\": all_top_inds,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9XSC9oBqNwLF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:59<00:00,  2.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': [['6', '4', '2', '0'],\n",
       "  ['2', '24', '8', '120'],\n",
       "  ['0', '0,1', '0,4', '1'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['6x^2 + 4x + 6', '2x^2 + 5', 'x^2 + 1', '0'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['4', '6', '2', '0'],\n",
       "  ['2,3', '2', '6', '1'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['an equivalence relation',\n",
       "   'both symmetric and anti-symmetric',\n",
       "   'symmetric only',\n",
       "   'anti-symmetric only'],\n",
       "  ['1', '2', '11', '5'],\n",
       "  ['(x + 1)(x − 4)(x − 2)',\n",
       "   '(x − 2)(x + 2)(x − 1)',\n",
       "   '(x + 1)(x + 4)(x − 2)',\n",
       "   '(x - 1)(x − 4)(x − 2)'],\n",
       "  ['12', '105', '6', '30'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['-i', 'i', '-1', '1'],\n",
       "  ['(3,6)', '(3,1)', '(1,6)', '(1,1)'],\n",
       "  ['identity element does not exist',\n",
       "   'multiplication is not a binary operation',\n",
       "   'zero has no inverse',\n",
       "   'multiplication is not associative'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['6x^2 + 4x + 6', '2x^2 + 5', '0', 'x^2 + 1'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['group',\n",
       "   'abelian group',\n",
       "   'semi group with identity',\n",
       "   'commutative semigroups with identity'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['finite abelian group',\n",
       "   'ininite, abelian',\n",
       "   'subgroup',\n",
       "   'infinite, non abelian group'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['No.', 'Yes, with p=2.', 'Yes, with p=3.', 'Yes, with p=5.'],\n",
       "  ['True, True', 'False, False', 'True, False', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['abelian group',\n",
       "   'non-abelian group',\n",
       "   'commutative semi group',\n",
       "   'None of these'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['(g o h)^2 = g^2 o h^2 for every g,h in G',\n",
       "   'G is of finite order',\n",
       "   'g = g^-1 for every g in G',\n",
       "   'g = g^2 for every g in G'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['6', '4', '2', '0'],\n",
       "  ['22', '11', '1', '0'],\n",
       "  ['Yes, with p=2.', 'Yes, with p=3.', 'Yes, with p=5.', 'No.'],\n",
       "  ['4', '1', '2', '3'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['1', '0', '-1', '12'],\n",
       "  ['3', '12', '30', '0'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'False, True', 'True, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'False, True', 'True, True'],\n",
       "  ['3', '2', '1', '0'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['240', '8', '24', '120'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['3', '2', '1', '0'],\n",
       "  ['2', '8', '6', '4'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['2', '1', '3', '0'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, True', 'True, True', 'False, False'],\n",
       "  ['1', '4', '2', '3'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['symmetric', 'reflexive', 'transitive', 'not anti-symmetric'],\n",
       "  ['2', 'infinitely many', '1', '0'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'False, True', 'True, True'],\n",
       "  ['a,b in H=> a * b in H',\n",
       "   'a,b in H=> a * b^-1 in H',\n",
       "   'a in H => a^-1 in H',\n",
       "   'H contains the identity element'],\n",
       "  ['(x − 2)(x + 2)(x − 1)(x + 1)', '(x+1)^4', '(x-1)^3(x+1)', '(x-1)(x+1)^3'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['(2+a)*-1', 'a-2', '-2', '0'],\n",
       "  ['6', '4', '2', '0'],\n",
       "  ['3', '4', '1', '2'],\n",
       "  ['3', '12', '30', '0'],\n",
       "  ['False, False', 'True, False', 'False, True', 'True, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['0', '12', '3', '30'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['False, False', 'True, False', 'True, True', 'False, True'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['19', '-10', '-19', '10'],\n",
       "  ['6', '4', '1', '0'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['True, True', 'True, False', 'False, False', 'False, True'],\n",
       "  ['4Z', '4Z, 2 + 4Z', 'Z', '2Z'],\n",
       "  ['Empty relation _ is reflexive',\n",
       "   'Properties of a relation being symmetric and being un-symmetric are negative of each other.',\n",
       "   'Every equivalence relation is a partial-ordering relation.',\n",
       "   'Number of relations form A = {x, y, z} to B= (1, 2), is 64.'],\n",
       "  ['6', '12', '105', '30'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['True, False', 'False, False', 'True, True', 'False, True'],\n",
       "  ['0', '0,1', '2', '1'],\n",
       "  ['30', '25', '6', '5'],\n",
       "  ['4', '6', '12', '8'],\n",
       "  ['True, False', 'True, True', 'False, False', 'False, True'],\n",
       "  ['paralysis of the facial muscles.',\n",
       "   'paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation.',\n",
       "   'paralysis of the facial muscles and loss of taste.',\n",
       "   'paralysis of the facial muscles, loss of taste and lacrimation.'],\n",
       "  ['a recessive maxilla due to failure of elongation of the cranial base.',\n",
       "   'defective development of the maxillary air sinus.',\n",
       "   'a protruding mandible due to reactivation of the condylar cartilage by acromegaly.',\n",
       "   'an enlarged frontal bone due to hydrocephaly.'],\n",
       "  ['Bladder', 'Kidney', 'Urethra', 'Ureter'],\n",
       "  ['Sweat glands', 'Skeletal muscles', 'Melanocytes', 'Motor neurons'],\n",
       "  ['Glomerulus', 'Loop of Henle', 'Afferent arteriole', 'Renal pelvis'],\n",
       "  ['It is impossible to predict an effect on breathing.',\n",
       "   'Breathing will be unaffected.',\n",
       "   'They will be unable to breathe without life support.',\n",
       "   'They will only be able to breathe quietly.'],\n",
       "  ['Epigastric', 'Hypogastric', 'Lumbar', 'Hypochondriac'],\n",
       "  ['Mucous membranes', 'Saliva', 'Hair in the nose', 'Osteoblasts'],\n",
       "  ['twelve weeks post-fertilization.',\n",
       "   'eight weeks post-fertilization.',\n",
       "   'six weeks post-fertilization.',\n",
       "   'ten weeks post-fertilization.'],\n",
       "  ['contraction of contralateral limb musculature.',\n",
       "   'contraction of ipsilateral limb musculature.',\n",
       "   'bilateral contraction of limb musculature.',\n",
       "   'bilateral contraction of antigravity limb muscles.'],\n",
       "  ['hard palate and upper lip.',\n",
       "   'hard palate, upper lip, upper central incisor and lower first molar.',\n",
       "   'hard palate, upper lip and upper central incisor.',\n",
       "   'hard palate.'],\n",
       "  ['Pupillary constriction and a medial strabismus',\n",
       "   'Pupillary dilatation and a medial strabismus',\n",
       "   'Pupillary constriction and a lateral strabismus',\n",
       "   'Pupillary dilatation and a lateral strabismus'],\n",
       "  ['Its articular surfaces are covered by hyaline cartilage',\n",
       "   'It has an articular disc of hyaline cartilage',\n",
       "   'Proprioceptive information is carried by the chorda tympani and deep temporal nerves',\n",
       "   'Proprioceptive information is carried by the masseteric and auriculotemporal nerves.'],\n",
       "  ['cricothyroid muscle is paralyzed but the interarytenoids are fully active.',\n",
       "   'cricothyroid muscle is still functional but the interarytenoids are weak.',\n",
       "   'cricothyroid muscle is still functional but the interarytenoid muscles are fully active.',\n",
       "   'cricothyroid muscle is paralyzed but the interarytenoids are weak.'],\n",
       "  [\"The ossification centre appears about 8 weeks I.U.L medial to Meckel's cartilage\",\n",
       "   'The functional matrix acting on the angle of the mandible is the lateral pterygoid muscle',\n",
       "   'The mental symphysis closes at about 1 year postnatally',\n",
       "   'The condylar growth cartilage ceases activity at puberty'],\n",
       "  ['Its afferent limb is carried in the oculomotor nerve',\n",
       "   'It is mediated by the inferior colliculi in the midbrain',\n",
       "   'Its efferent limb is carried in the optic nerve',\n",
       "   'It is a consensual reflex'],\n",
       "  ['Internal intercostal muscles and diaphragm',\n",
       "   'External and internal intercostal muscles and diaphragm',\n",
       "   'External intercostal muscles and diaphragm',\n",
       "   'External and internal intercostal muscles'],\n",
       "  ['Prostate', 'Testes', 'Prepuce', 'Glans penis'],\n",
       "  ['Aorta', 'Pulmonary veins', 'Carotid arteries', 'Inferior vena cava'],\n",
       "  ['Trachea', 'Aorta', 'Esophagus', 'Pancreas'],\n",
       "  ['Between the right atrium and the right ventricle',\n",
       "   'Between the left atrium and the left ventricle',\n",
       "   'In the upper wall of the left ventricle',\n",
       "   'In the upper wall of the right atrium'],\n",
       "  ['superficial to its inferior border.',\n",
       "   'superficial to its superior border.',\n",
       "   'deep to its inferior border.',\n",
       "   'deep to its superior border.'],\n",
       "  ['left submandibular lymph node.',\n",
       "   'left submental lymph node.',\n",
       "   'left and right submandibular lymph nodes.',\n",
       "   'left and right submental lymph nodes.'],\n",
       "  ['mesoderm formation and occurs before neurulation.',\n",
       "   'ectomesenchyme formation and occurs before neurulation.',\n",
       "   'mesoderm formation and occurs after neurulation.',\n",
       "   'ectomesenchyme formation and occurs after neurulation.'],\n",
       "  ['is a secondary growth cartilage.',\n",
       "   'influences the position of the viscerocranium.',\n",
       "   'can be reactivated in patients affected by acromegaly.',\n",
       "   'ceases activity at 7 years of age.'],\n",
       "  ['Carbon dioxide', 'Oxygen', 'Nitrogen', 'Carbon monoxide'],\n",
       "  ['Nephrolithiasis',\n",
       "   'Glomerulonephritis',\n",
       "   'Polycystic kidney',\n",
       "   'Interstitial nephritis'],\n",
       "  ['facial artery crosses the maxilla.',\n",
       "   'superficial temporal artery crosses the maxilla.',\n",
       "   'facial artery crosses the mandible.',\n",
       "   'superficial temporal artery crosses the orbital rim.'],\n",
       "  ['Spleen', 'Gallbladder', 'Appendix', 'Urinary bladder'],\n",
       "  ['It is formed of autonomic nerve processes and forms one of several electrical connections between the atria and ventricles.',\n",
       "   'It is formed of Purkinje fibres and forms one of several electrical connections between the atria and ventricles.',\n",
       "   'It is formed of autonomic nerve processes and is the only electrical connection between the atria and the ventricles.',\n",
       "   'It is formed of Purkinje fibres and is the only electrical connection between the atria and the ventricles'],\n",
       "  ['Flexion', 'Abduction', 'Eversion', 'Pronation'],\n",
       "  ['constriction of the laryngeal entrance.',\n",
       "   'downward movement of the larynx.',\n",
       "   'upward movement of the epiglottis.',\n",
       "   'increase in respiratory rate.'],\n",
       "  ['Ileocecum', 'Gastroesophageal sphincter', 'Jejunum', 'Duodenum'],\n",
       "  ['Lateral', 'Prone', 'Dorsal', 'Erect'],\n",
       "  ['palatine processes.',\n",
       "   'palatine, frontonasal, secondary nasal processes and mandibular processes.',\n",
       "   'palatine and frontonasal processes.',\n",
       "   'palatine, frontonasal and secondary nasal processes.'],\n",
       "  ['connects the inner ear and nasopharynx.',\n",
       "   'is closed by the action of tensor veli palatini.',\n",
       "   'is opened by the action of levator veli palatini.',\n",
       "   'is derived from the second pharyngeal pouch.'],\n",
       "  ['Spleen', 'Gallbladder', 'Appendix', 'Pancreas'],\n",
       "  ['Pericardial and pleural',\n",
       "   'Thoracic and abdominal',\n",
       "   'Abdominal and pelvic',\n",
       "   'Cranial and spinal'],\n",
       "  ['smooth muscle and respiratory epithelium.',\n",
       "   'cartilage, smooth muscle and respiratory epithelium.',\n",
       "   'cartilage and respiratory epithelium.',\n",
       "   'cartilage and smooth muscle.'],\n",
       "  ['Liver', 'Kidney', 'Gallbladder', 'Spleen'],\n",
       "  ['Pyloric stenosis',\n",
       "   'Gastroesophageal reflux disease',\n",
       "   \"Crohn's disease\",\n",
       "   'Esophageal varices'],\n",
       "  ['facial and glossopharyngeal nerves.',\n",
       "   'trigeminal and glossopharyngeal nerves.',\n",
       "   'facial and vagus nerves.',\n",
       "   'trigeminal and vagus nerves.'],\n",
       "  ['Femur', 'Symphysis pubis', 'Calcaneus', 'Tibia'],\n",
       "  ['lower lip.',\n",
       "   'lower lip and mandibular teeth.',\n",
       "   'lower lip, mandibular teeth and labial gingivae of the anterior and buccal gingivae of the posterior mandibular teeth.',\n",
       "   'lower lip, mandibular teeth and labial gingivae of the anterior mandibular teeth.'],\n",
       "  ['internal laryngeal nerve which is the afferent limb of the gag reflex.',\n",
       "   'internal laryngeal nerve which is the afferent limb of the cough reflex.',\n",
       "   'external laryngeal nerve which is the afferent limb of the gag reflex.',\n",
       "   'external laryngeal nerve which is the afferent limb of the cough reflex.'],\n",
       "  ['intramembranous ossification and follow the neural growth pattern.',\n",
       "   'intramembranous ossification and follow the somatic growth pattern.',\n",
       "   'endochondral ossification and follow the somatic growth pattern.',\n",
       "   'endochondral ossification and follow the neural growth pattern.'],\n",
       "  ['Lower leg', 'Thigh', 'Toes', 'Heel'],\n",
       "  ['Erythrocyte', 'Monocyte', 'Lymphocyte', 'Basophil'],\n",
       "  ['frontal and parietal bones.',\n",
       "   'parietal, squamous temporal and greater wing of the sphenoid.',\n",
       "   'left and right parietal bones.',\n",
       "   'parietal and occipital bones.'],\n",
       "  ['hyoid bone, cricoid cartilage, thyroid cartilage.',\n",
       "   'hyoid bone, thyroid cartilage, cricoid cartilage.',\n",
       "   'thyroid cartilage, hyoid bone, cricoids cartilage.',\n",
       "   'thyroid cartilage, cricoid cartilage, hyoid bone.'],\n",
       "  ['All of the above',\n",
       "   'The infraorbital foramen',\n",
       "   'The supraorbital foramen',\n",
       "   'The mental foramen'],\n",
       "  ['The sympathetic trunks',\n",
       "   'The vagus nerves',\n",
       "   'The phrenic nerves',\n",
       "   'The splanchnic nerves'],\n",
       "  ['Left ventricle → aorta → arteries →capillaries → veins',\n",
       "   'Right ventricle → pulmonary trunk → arteries → capillaries →veins',\n",
       "   'Right ventricle → pulmonary trunk → arteries → veins → capillaries',\n",
       "   'Left ventricle → aorta → arteries → veins → capillaries'],\n",
       "  ['transverse', 'dorsal', 'frontal', 'caudal'],\n",
       "  ['deep to masseter and enters the mouth opposite the upper second molar.',\n",
       "   'superficial to masseter and enters the mouth opposite the upper second molar.',\n",
       "   'deep to masseter and enters the mouth opposite the upper second premolar.',\n",
       "   'superficial to masseter and enters the mouth opposite the upper second premolar.'],\n",
       "  ['Downward', 'Inward', 'Upward', 'Outward'],\n",
       "  ['is a terminal branch of the maxillary division of the trigeminal nerve.',\n",
       "   'is a terminal branch of the maxillary branch of the facial nerve.',\n",
       "   'carries parasympathetic secretomotor neurons to the lacrimal gland.',\n",
       "   'innervates the inferior part of the orbicularis oculi muscle.'],\n",
       "  ['Peristalsis', 'Absorption', 'Emulsion', 'Regurgitation'],\n",
       "  ['Synapse', 'Nerve center', 'Dendrite', 'Glial cell'],\n",
       "  ['Lateral pterygoid', 'Temporalis', 'Medial pterygoid.', 'Masseter'],\n",
       "  ['Pancreas', 'Adrenal gland', 'Liver', 'Gallbladder'],\n",
       "  ['loss of somaesthetic sensation over the anterior two thirds of the tongue.',\n",
       "   'loss of taste and somaesthetic sensation over the posterior third of the tongue.',\n",
       "   'paralysis of the muscles of the tongue.',\n",
       "   'loss of taste over the anterior two-thirds of the tongue.'],\n",
       "  ['bitemporal hemianopia.',\n",
       "   'ptosis.',\n",
       "   'pupillary contriction.',\n",
       "   'a convergent strabismus.'],\n",
       "  ['Left ventricle',\n",
       "   'Pulmonary arteries',\n",
       "   'Pulmonary veins',\n",
       "   'Inferior vena cava'],\n",
       "  ['extend their neck against resistance.',\n",
       "   'lift their shoulders against resistance.',\n",
       "   'extend their neck without impairment.',\n",
       "   'lift their shoulders without impairment.'],\n",
       "  ['The muscles of mastication, anterior and posterior bellies of digastric and geniohyoid',\n",
       "   'The muscles of mastication and anterior and posterior bellies of digastric',\n",
       "   'The muscles of mastication and anterior belly of digastric',\n",
       "   'The muscles of mastication'],\n",
       "  ['On the anterior side of the neck',\n",
       "   'In front of the ears and just above eye level',\n",
       "   'In the antecubital space',\n",
       "   'In the middle of the groin'],\n",
       "  ['nociception decussate in the medial lemniscus',\n",
       "   'skilled movements decussate in the medial lemniscus',\n",
       "   'discriminative touch decussate in the pyramids.',\n",
       "   'skilled motor movements decussate in the pyramids.'],\n",
       "  ['cricothyroid membrane.',\n",
       "   'tracheal rings.',\n",
       "   'cricoid cartilage.',\n",
       "   'thyroid cartilage.'],\n",
       "  ['The incisive nerve',\n",
       "   'The buccal nerve',\n",
       "   'The lingual nerve',\n",
       "   'The mental nerve'],\n",
       "  ['the articular disc of the TMJ and the medial pterygoid muscle.',\n",
       "   'the articular disc of the TMJ and the lateral pterygoid muscle.',\n",
       "   'the capsule and ligaments of the TMJ and the medial pterygoid muscle.',\n",
       "   'the capsule and ligaments of the TMJ and the lateral pterygoid muscle.'],\n",
       "  ['Acetylcholine', 'Cholecystokinin', 'Oxytocin', 'Deoxyribose'],\n",
       "  ['Lamina IX', 'Lamina V.', 'Lamina I.', 'Lamina II.'],\n",
       "  ['posterior fragment to be displaced anteriorly by the action of the lateral pterygoid muscle.',\n",
       "   'anterior fragment to be displaced downwards by the action of the digastric muscle.',\n",
       "   'anterior fragment to be displaced backwards by action of the temporalis muscle.',\n",
       "   'posterior fragment to be displaced medially by action of the medial pterygoid muscle.'],\n",
       "  ['ophthalmic trigeminal nerve.',\n",
       "   'oculomotor nerve.',\n",
       "   'abducens nerve.',\n",
       "   'trochlear nerve.'],\n",
       "  ['atrophy', 'flaccid paralysis', 'contracture', 'spastic paralysis'],\n",
       "  ['hyporeflexia.', 'spastic paralysis.', 'flaccid paralysis.', 'tremor.'],\n",
       "  ['right lung because the right main bronchus is wider and more vertical than the left.',\n",
       "   'left lung because the left main bronchus is wider and more vertical than the right.',\n",
       "   'right or left lung because there are no factors predisposing to the fragment going in one particular direction.',\n",
       "   'right lung or left lung because the two main bronchi are of equal size and at the same angulation.'],\n",
       "  ['Electrolytes', 'Androgens', 'Catecholamines', 'Estrogens'],\n",
       "  ['sympathetic pre- and post-ganglionic neurons',\n",
       "   'sympathetic post-ganglionic neurons and target organs',\n",
       "   'parasympathetic post-ganglionic neurons and target organs',\n",
       "   'parasympathetic pre- and post-ganglionic neurons'],\n",
       "  ['Adrenal', 'Bulbourethral', 'Corpus luteum', \"Bartholin's\"],\n",
       "  ['inferior to the cricoid cartilage.',\n",
       "   'superior to the thyroid isthmus.',\n",
       "   'inferior to the thyroid cartilage.',\n",
       "   'superior to the thyroid cartilage.'],\n",
       "  ['pia mater and brain surface.',\n",
       "   'dura mater and arachnoid mater.',\n",
       "   'skull and dura mater.',\n",
       "   'arachnoid and pia maters.'],\n",
       "  ['Larynx', 'Epiglottis', 'Uvula', 'Alveoli'],\n",
       "  ['The medial wall', 'The lateral wall', 'The floor', 'The roof'],\n",
       "  ['the floor of the orbit, the upper posterior teeth, the infratemporal fossa and the hard palate.',\n",
       "   'the floor of the orbit, the upper posterior teeth and the infratemporal fossa.',\n",
       "   'the floor of the orbit and the upper posterior teeth.',\n",
       "   'the floor of the orbit.'],\n",
       "  ['The zygomaticomaxillary sutures contribute to increase in height and length of the maxilla',\n",
       "   'Space is created for the eruption of the permanent molars by resorption of the maxillary tuberosity',\n",
       "   'Pneumatisation by enlargement of the developing maxillary sinus ceases at puberty',\n",
       "   'The intermaxillary suture closes about eight years postnatally'],\n",
       "  ['taste buds on the posterior third of the tongue.',\n",
       "   'muscles of the soft palate.',\n",
       "   'muscles of the lower lip.',\n",
       "   'the parotid salivary gland.'],\n",
       "  ['The anterior ethmoidal, frontal, maxillary and sphenoidal sinuses',\n",
       "   'The anterior ethmoidal, frontal and maxillary sinuses',\n",
       "   'The anterior ethmoidal sinuses',\n",
       "   'The anterior ethmoidal and frontal sinuses'],\n",
       "  ['Collagen', 'Melanin', 'Heparin', 'Lipocyte'],\n",
       "  ['hydrophilic molecules increasing turgor pressure in the palatine shelves.',\n",
       "   'a combination of these three processes.',\n",
       "   'descent of the tongue.',\n",
       "   'changes in flexure of the developing brain.'],\n",
       "  ['The buccal surfaces of the upper molars and lingual surfaces of the lower incisors.',\n",
       "   'The palatal surfaces of the upper molars and lingual surfaces of the lower incisors.',\n",
       "   'The buccal surfaces of the upper molars and labial surfaces of the lower incisors.',\n",
       "   'The palatal surfaces of the upper molars and labial surfaces of the lower incisors.'],\n",
       "  ['the mucosa above the vocal folds is more vascular than that below the vocal folds.',\n",
       "   'the mucosa covering the vocal folds is tightly attached to underlying tissues.',\n",
       "   'there are numerous mucous glands in the vestibular folds.',\n",
       "   'fluid will drain rapidly into the thorax below the vocal folds.'],\n",
       "  ['arise from the arch of the aorta and fill during systole.',\n",
       "   'arise from the ascending aorta and fill during systole.',\n",
       "   'arise from the arch of the aorta and fill during diastole.',\n",
       "   'arise from the ascending aorta and fill during diastole.'],\n",
       "  ['Right lateral pterygoid muscle',\n",
       "   'Right medial pterygoid muscle',\n",
       "   'Left medial pterygoid muscle',\n",
       "   'Left lateral pterygoid muscle'],\n",
       "  ['diaphragm, parietal pleura and pericardium.',\n",
       "   'diaphragm and parietal pleura.',\n",
       "   'diaphragm.',\n",
       "   'diaphragm, parietal pleura, pericardium and intercostals muscles.'],\n",
       "  ['third cranial nerves.',\n",
       "   'fourth cranial nerves.',\n",
       "   'fifth cranial nerves.',\n",
       "   'sixth cranial nerves.'],\n",
       "  ['red below the mucogingival junction and light pink above it.',\n",
       "   'light pink below the mucogingival junction and red above it.',\n",
       "   'light pink in colour on both sides of the mucogingigival junction.',\n",
       "   'red on both sides of the mucogingival junction.'],\n",
       "  ['maxillary and frontonasal processes.',\n",
       "   'maxillary and mandibular processes.',\n",
       "   'mandibular and hyoid arches.',\n",
       "   'left and right mandibular processes.'],\n",
       "  ['to be palpable intraorally..',\n",
       "   'to be palpable extraorally.',\n",
       "   'to be palpable both intra- and extraorally.',\n",
       "   'only to be detectable by radiographical examination.'],\n",
       "  ['deposit bone and differentiate from monocytes.',\n",
       "   'resorb bone and differentiate from monocytes.',\n",
       "   'resorb bone and differentiate from periosteal mesenchymal cells.',\n",
       "   'deposit bone and differentiate from periosteal mesenchymal cells.'],\n",
       "  ['is attached to the coronoid process and elevates the mandible.',\n",
       "   'is attached to the condylar process and elevates the mandible.',\n",
       "   'is attached to the coronoid process and protrudes the mandible.',\n",
       "   'is attached to the condylar process and protrudes the mandible.'],\n",
       "  ['aorta to the pulmonary artery.',\n",
       "   'pulmonary artery to the aorta.',\n",
       "   'pulmonary vein to the aorta.',\n",
       "   'aorta to pulmonary vein.'],\n",
       "  ['The ribs and sternum',\n",
       "   'The ribs, sternum and clavicle',\n",
       "   'The ribs, sternum, clavicle and vertebrae',\n",
       "   'The ribs'],\n",
       "  ['keratinised and lacks submucosa and minor salivary glands.',\n",
       "   'non-keratinised and lacks submucosa and minor salivary glands.',\n",
       "   'keratinised and has submucosa and minor salivary glands posterolaterally.',\n",
       "   'non-keratinised and has submucosa and minor salivary glands posteromedially.'],\n",
       "  ['The inferior articular demifacet of T5 and superior articular facet of T6.',\n",
       "   'The inferior articular demifacet of T5 and the superior articular demifacet of T6.',\n",
       "   'The inferior articular facet of T5 and superior articular facet of T6.',\n",
       "   'The superior and inferior demifacets of T6.'],\n",
       "  ['Spleen', 'Appendix', 'Pancreas', 'Duodenum'],\n",
       "  ['articular cartilage and synovial membrane.',\n",
       "   'synovial membrane and capsule.',\n",
       "   'ligaments and articular discs.',\n",
       "   'capsule and ligaments.'],\n",
       "  ['Mentation', 'Respiration', 'Alimentation', 'Menstruation'],\n",
       "  ['skull bones and dura mater.',\n",
       "   'pia mater and brain surface.',\n",
       "   'dura mater and arachnoid.',\n",
       "   'arachnoid and pia mater.'],\n",
       "  ['maxillary anterior teeth, their labial gingivae and the upper lip.',\n",
       "   'maxillary anterior teeth, their labial gingivae, the upper lip and anterior hard palate.',\n",
       "   'maxillary anterior teeth.',\n",
       "   'maxillary anterior teeth and their labial gingivae.'],\n",
       "  ['The foramen ovale, rotundum and spinosum and foramen lacerum',\n",
       "   'The foramen ovale, rotundum and spinosum',\n",
       "   'The foramen ovale and rotundum',\n",
       "   'The foramen ovale'],\n",
       "  ['Narcolepsy', 'Epilepsy', 'Dyslexia', 'Hydrocephalus'],\n",
       "  ['Bronchioles', 'Bronchi', 'Capillaries', 'Alveoli'],\n",
       "  ['Complex, comminuted',\n",
       "   'Compound, transverse',\n",
       "   'Open, spiral',\n",
       "   'Closed, greenstick'],\n",
       "  ['Ovary', 'Cervix', 'Fallopian tube', 'Uterus'],\n",
       "  ['all three structures.',\n",
       "   'ectomesenchymal cells.',\n",
       "   'the neural tube.',\n",
       "   'dental enamel.'],\n",
       "  ['Prostate', 'Seminal vesicle', 'Scrotum', 'Epididymis'],\n",
       "  ['is open during systole.',\n",
       "   'prevents blood returning from the pulmonary trunk as the heart relaxes.',\n",
       "   'is a semilunar valve',\n",
       "   'is prevented from everting by papillary muscles.'],\n",
       "  ['The frontal lobe',\n",
       "   'The parietal lobe',\n",
       "   'The cerebellum',\n",
       "   'The hypothalamus'],\n",
       "  ['Cecum', 'Ileum', 'Ascending colon', 'Sigmoid colon'],\n",
       "  ['inferior to medial pterygoid.',\n",
       "   'posterior and medial to medial pterygoid.',\n",
       "   'through medial pterygoid.',\n",
       "   'anterior and lateral to medial pterygoid.'],\n",
       "  ['Pernicious anemia', \"Crohn's disease\", \"Bell's palsy\", \"Graves' disease\"],\n",
       "  ['The maxillary bone',\n",
       "   'The sphenoid bone',\n",
       "   'The frontal bone',\n",
       "   'The temporal bone'],\n",
       "  ['thoracic spinal nerves.',\n",
       "   'sacral spinal nerves.',\n",
       "   'lumbar spinal nerves.',\n",
       "   'cervical spinal nerves.'],\n",
       "  ['Pupillary constriction, vasodilation of facial vessels, increased facial sweating and decreased lacrimation',\n",
       "   'Pupillary constriction and vasodilation of facial vessels',\n",
       "   'Pupillary constriction, vasodilation of facial vessels and increased facial sweating',\n",
       "   'Pupillary constriction'],\n",
       "  ['Olfactory', 'Hypoglossal', 'Trochlear', 'Abducens'],\n",
       "  ['midbrain', 'medulla oblongata', 'cerebellum', 'pons'],\n",
       "  ['Fats', 'Minerals', 'Starches', 'Proteins'],\n",
       "  ['rima glottis opens.',\n",
       "   'cricopharyngeus muscle contracts.',\n",
       "   'gag reflex is suspended.',\n",
       "   'auditory tube opens.'],\n",
       "  ['sensory and autonomic neuronal processes.',\n",
       "   'motor and autonomic neuronal processes.',\n",
       "   'sensory neuronal processes.',\n",
       "   'motor neuronal processes.'],\n",
       "  ['Pituitary', 'Pancreas', 'Adrenal', 'Pineal'],\n",
       "  ['hypoglossal nerve.',\n",
       "   'chorda tympani branch of the facial nerve.',\n",
       "   'lingual branch of the glossopharyngeal nerve.',\n",
       "   'lingual branch of the mandibular trigeminal nerve.'],\n",
       "  ['Anus', 'Urethra', 'Testicle', 'Bladder'],\n",
       "  ['The mandibular canal, maxillary sinus, hard palate and zygomatic arch',\n",
       "   'The mandibular canal, maxillary sinus and hard palate',\n",
       "   'The mandibular canal and maxillary sinus',\n",
       "   'The mandibular canal'],\n",
       "  ['This type occurs in young galaxies.',\n",
       "   'This type occurs in binary systems.',\n",
       "   'This type produces gamma-ray bursts.',\n",
       "   'This type produces high amounts of X-rays.'],\n",
       "  ['Its distance from you',\n",
       "   'Its size',\n",
       "   'Its speed relative to you',\n",
       "   'Its composition'],\n",
       "  [\"Because the sky reflects the color of the Earth's oceans.\",\n",
       "   \"Because the molecules that compose the Earth's atmosphere have a blue-ish color.\",\n",
       "   \"Because the Earth's atmosphere preferentially absorbs all other colors.\",\n",
       "   'Because the atmosphere preferentially scatters short wavelengths.'],\n",
       "  ['You can never prove your theory to be correct only “yet to be proven wrong”.',\n",
       "   'When you and many others have tested the hypothesis.',\n",
       "   'After you’ve repeated your experiment many times.',\n",
       "   'The first time you drop a bowling ball and it falls to the ground proving your hypothesis.'],\n",
       "  ['Titan is the only outer solar system moon with evidence for recent geologic activity',\n",
       "   'Titan is the only outer solar system moon with a thick atmosphere',\n",
       "   \"Titan's atmosphere is composed mostly of hydrocarbons\",\n",
       "   'A and D'],\n",
       "  ['m*v', '0.5*m*(2GM/R)', '0.6*G(M^2)/R', '0.5*m/(R^3)'],\n",
       "  ['it was broken into hydrogen and oxygen by ultraviolet light and the hydrogen was lost through thermal escape',\n",
       "   'it was vaporized during a period of intense volcanism and was lost to space through thermal escape',\n",
       "   'it was consumed by a civilization of thirsty Martians',\n",
       "   'it was stripped away by the magnetic field when it rapidly decreased in strength'],\n",
       "  ['Radioactive dating of lunar samples shows that they are older.',\n",
       "   \"The Moon's surface has more impact craters than the Earth's surface.\",\n",
       "   \"The Moon's surface is more heavily eroded than the Earth's surface.\",\n",
       "   'Lunar rocks are composed of fragments pulverized by many impacts.'],\n",
       "  ['Helium', 'Hydrogen', 'Iron', 'Methane'],\n",
       "  [\"The nuclei of comets gradually disintegrate and spread out along the comet's orbital path. When the Earth passes through the orbit of a comet we are bombarded by sand-sized particles which cause a meteor shower.\",\n",
       "   'Near-Earth asteroids gradually disintegrate and spread out along their orbital path. When the Earth passes through the orbit of an asteroid we are bombarded by sand-sized particles which cause a meteor shower.',\n",
       "   \"The nuclei of comets disintigrate as they enter Earth's atmosphere creating hundreds of bright meteors that appear to radiate from a central location in the sky.\",\n",
       "   \"Near-Earth asteroids disintegrate as they enter Earth's atmosphere creating hundreds of bright meteors that appear to radiate from a single location in the sky.\"],\n",
       "  ['The conservation of the angular momentum in the tail keeps it always pointing away from the Sun.',\n",
       "   'The ions are following Keplerian orbits that lead away from the Sun.',\n",
       "   \"Radiation pressure from the Sun's light pushes the ions away.\",\n",
       "   'The solar wind electromagnetically \"blows\" the ions directly away from the Sun.'],\n",
       "  [\"By measuring the size of Earth's shadow on the Moon in a lunar eclipse.\",\n",
       "   'By comparing the maximum altitude of the Sun in two cities at different latitudes at the same time on the same day.',\n",
       "   'By observing the duration of a solar eclipse.',\n",
       "   'By sending fleets of ships around Earth.'],\n",
       "  ['the outer arm of the Milky Way named after Magellan.',\n",
       "   'a bright star cluster discovered by Magellan.',\n",
       "   'a dwarf galaxy orbiting the Milky Way.',\n",
       "   'the closest planetary nebula to the Earth.'],\n",
       "  ['These craters contain the only permanently shadowed regions on Mercury',\n",
       "   'Actually water-ice is all over Mercury and not just at the poles.',\n",
       "   'The pole is the only place fortunate enough to have had comet impacts',\n",
       "   \"Radar from the earth can only see Mercury's poles.\"],\n",
       "  ['Laniakea', 'Virgo', 'Sculptor', 'Boötes'],\n",
       "  ['Mars', 'Saturn', 'Venus', 'Jupiter'],\n",
       "  ['It is the only one that has both a partially molten metallic core and reasonably rapid rotation.',\n",
       "   'It is the most volcanically active world.',\n",
       "   'It is by far the largest terrestrial world.',\n",
       "   'It rotates much faster than any other terrestrial world.'],\n",
       "  ['Neptune and Pluto are in a 3:2 orbital resonance (for every 3 Neptune orbits there are exactly 2 Pluto orbits)',\n",
       "   'The Earth and Venus are in a 1:1 orbital resonance (for every 1 Earth orbit there is exactly 1 Venus orbit)',\n",
       "   'The Kirkwood Gaps in the asteroid belt are due to resonances with Jupiter',\n",
       "   \"Neptune and Pluto won't collide because of their orbital resonance\"],\n",
       "  ['ejecta', 'raised rims', 'central peaks', 'A and B only'],\n",
       "  ['The star is getting hotter.',\n",
       "   'The star is moving away from us.',\n",
       "   'The star is getting colder.',\n",
       "   'The star is moving toward us.'],\n",
       "  ['the rigid rocky material of the crust and uppermost portion of the mantle.',\n",
       "   'material between the crust and the mantle.',\n",
       "   'the lava that comes out of volcanoes.',\n",
       "   'the softer rocky material of the mantle.']],\n",
       " 'scores': [[0.27904778718948364,\n",
       "   0.27592524886131287,\n",
       "   0.23753005266189575,\n",
       "   0.20749692618846893],\n",
       "  [0.2823728621006012,\n",
       "   0.27395421266555786,\n",
       "   0.2673807144165039,\n",
       "   0.17629222571849823],\n",
       "  [0.3203989565372467,\n",
       "   0.2648690938949585,\n",
       "   0.20900192856788635,\n",
       "   0.20572997629642487],\n",
       "  [0.32078590989112854,\n",
       "   0.3123796582221985,\n",
       "   0.20691782236099243,\n",
       "   0.15991663932800293],\n",
       "  [0.30146196484565735,\n",
       "   0.2760325074195862,\n",
       "   0.22817426919937134,\n",
       "   0.19433124363422394],\n",
       "  [0.2891002893447876,\n",
       "   0.27713069319725037,\n",
       "   0.24595104157924652,\n",
       "   0.1878180205821991],\n",
       "  [0.33411112427711487,\n",
       "   0.2761334478855133,\n",
       "   0.22063907980918884,\n",
       "   0.16911634802818298],\n",
       "  [0.2829088270664215,\n",
       "   0.2645503878593445,\n",
       "   0.23793011903762817,\n",
       "   0.21461062133312225],\n",
       "  [0.2754397690296173,\n",
       "   0.2753339409828186,\n",
       "   0.24551188945770264,\n",
       "   0.20371438562870026],\n",
       "  [0.309716135263443,\n",
       "   0.26225975155830383,\n",
       "   0.22154995799064636,\n",
       "   0.2064741849899292],\n",
       "  [0.3198534846305847,\n",
       "   0.24350187182426453,\n",
       "   0.23918338119983673,\n",
       "   0.19746126234531403],\n",
       "  [0.3268735706806183,\n",
       "   0.3070027828216553,\n",
       "   0.2262861281633377,\n",
       "   0.13983750343322754],\n",
       "  [0.27910149097442627,\n",
       "   0.24409720301628113,\n",
       "   0.23858679831027985,\n",
       "   0.23821455240249634],\n",
       "  [0.2580587863922119,\n",
       "   0.2541535496711731,\n",
       "   0.24509704113006592,\n",
       "   0.24269062280654907],\n",
       "  [0.29861781001091003,\n",
       "   0.24961312115192413,\n",
       "   0.24502936005592346,\n",
       "   0.2067396640777588],\n",
       "  [0.3313879668712616,\n",
       "   0.28061434626579285,\n",
       "   0.20261117815971375,\n",
       "   0.18538644909858704],\n",
       "  [0.45819252729415894,\n",
       "   0.2197454273700714,\n",
       "   0.18219229578971863,\n",
       "   0.13986976444721222],\n",
       "  [0.33510294556617737,\n",
       "   0.2369856983423233,\n",
       "   0.21483798325061798,\n",
       "   0.21307331323623657],\n",
       "  [0.34121832251548767,\n",
       "   0.24828429520130157,\n",
       "   0.2074471116065979,\n",
       "   0.20305025577545166],\n",
       "  [0.30807772278785706,\n",
       "   0.3062995672225952,\n",
       "   0.21764062345027924,\n",
       "   0.16798216104507446],\n",
       "  [0.28488385677337646,\n",
       "   0.2694511413574219,\n",
       "   0.26882871985435486,\n",
       "   0.1768363118171692],\n",
       "  [0.3056871294975281,\n",
       "   0.25680288672447205,\n",
       "   0.2271432727575302,\n",
       "   0.21036674082279205],\n",
       "  [0.28334495425224304,\n",
       "   0.2703776955604553,\n",
       "   0.26167455315589905,\n",
       "   0.1846027821302414],\n",
       "  [0.5890584588050842,\n",
       "   0.29431983828544617,\n",
       "   0.07831490784883499,\n",
       "   0.03830674663186073],\n",
       "  [0.30927199125289917,\n",
       "   0.27349305152893066,\n",
       "   0.24335405230522156,\n",
       "   0.17388087511062622],\n",
       "  [0.2874907851219177,\n",
       "   0.28734639286994934,\n",
       "   0.2534521520137787,\n",
       "   0.17171062529087067],\n",
       "  [0.31497156620025635,\n",
       "   0.27741846442222595,\n",
       "   0.23494142293930054,\n",
       "   0.17266853153705597],\n",
       "  [0.25998780131340027,\n",
       "   0.25438040494918823,\n",
       "   0.24475297331809998,\n",
       "   0.24087882041931152],\n",
       "  [0.33037641644477844,\n",
       "   0.268828809261322,\n",
       "   0.23900681734085083,\n",
       "   0.16178792715072632],\n",
       "  [0.4936777353286743,\n",
       "   0.18819430470466614,\n",
       "   0.17532296478748322,\n",
       "   0.1428050547838211],\n",
       "  [0.2759426534175873,\n",
       "   0.2693292796611786,\n",
       "   0.2346479594707489,\n",
       "   0.22008007764816284],\n",
       "  [0.5061295032501221,\n",
       "   0.1995348185300827,\n",
       "   0.19848082959651947,\n",
       "   0.09585485607385635],\n",
       "  [0.295082688331604,\n",
       "   0.29015541076660156,\n",
       "   0.23995760083198547,\n",
       "   0.17480427026748657],\n",
       "  [0.30489030480384827,\n",
       "   0.296650230884552,\n",
       "   0.23016011714935303,\n",
       "   0.16829930245876312],\n",
       "  [0.27894049882888794,\n",
       "   0.2724382281303406,\n",
       "   0.2602720260620117,\n",
       "   0.18834926187992096],\n",
       "  [0.2758144736289978,\n",
       "   0.2733316123485565,\n",
       "   0.24216139316558838,\n",
       "   0.2086925059556961],\n",
       "  [0.3080584704875946,\n",
       "   0.23573428392410278,\n",
       "   0.23025231063365936,\n",
       "   0.22595490515232086],\n",
       "  [0.2724238336086273,\n",
       "   0.24865126609802246,\n",
       "   0.2395879477262497,\n",
       "   0.2393370270729065],\n",
       "  [0.25515085458755493,\n",
       "   0.2547239363193512,\n",
       "   0.2490442842245102,\n",
       "   0.24108090996742249],\n",
       "  [0.2910096347332001,\n",
       "   0.2897840440273285,\n",
       "   0.22298727929592133,\n",
       "   0.19621902704238892],\n",
       "  [0.4528941512107849,\n",
       "   0.21006564795970917,\n",
       "   0.19790709018707275,\n",
       "   0.13913306593894958],\n",
       "  [0.3482675850391388,\n",
       "   0.23991483449935913,\n",
       "   0.21744577586650848,\n",
       "   0.19437171518802643],\n",
       "  [0.32050183415412903,\n",
       "   0.2498241364955902,\n",
       "   0.23414510488510132,\n",
       "   0.19552889466285706],\n",
       "  [0.33974698185920715,\n",
       "   0.24270161986351013,\n",
       "   0.22860482335090637,\n",
       "   0.18894657492637634],\n",
       "  [0.2777257263660431,\n",
       "   0.26621830463409424,\n",
       "   0.2390553206205368,\n",
       "   0.21700064837932587],\n",
       "  [0.32968735694885254,\n",
       "   0.2651168406009674,\n",
       "   0.22651050984859467,\n",
       "   0.17868533730506897],\n",
       "  [0.2948792576789856,\n",
       "   0.290495365858078,\n",
       "   0.2729332745075226,\n",
       "   0.141692116856575],\n",
       "  [0.32304301857948303,\n",
       "   0.23703482747077942,\n",
       "   0.23437367379665375,\n",
       "   0.2055485099554062],\n",
       "  [0.2914092540740967,\n",
       "   0.2661476135253906,\n",
       "   0.26054900884628296,\n",
       "   0.18189416825771332],\n",
       "  [0.3014240264892578,\n",
       "   0.2427181601524353,\n",
       "   0.23850908875465393,\n",
       "   0.21734867990016937],\n",
       "  [0.29077276587486267,\n",
       "   0.2764779031276703,\n",
       "   0.2494283765554428,\n",
       "   0.18332090973854065],\n",
       "  [0.34254229068756104,\n",
       "   0.2358626276254654,\n",
       "   0.2204541563987732,\n",
       "   0.20114095509052277],\n",
       "  [0.3023327887058258,\n",
       "   0.2962109446525574,\n",
       "   0.2266860157251358,\n",
       "   0.1747703105211258],\n",
       "  [0.3132364749908447,\n",
       "   0.26907259225845337,\n",
       "   0.22794756293296814,\n",
       "   0.18974332511425018],\n",
       "  [0.2669306993484497,\n",
       "   0.26535579562187195,\n",
       "   0.25260359048843384,\n",
       "   0.21510985493659973],\n",
       "  [0.3132658302783966,\n",
       "   0.281737744808197,\n",
       "   0.22479292750358582,\n",
       "   0.18020348250865936],\n",
       "  [0.42272061109542847,\n",
       "   0.2160501331090927,\n",
       "   0.1958494484424591,\n",
       "   0.1653798371553421],\n",
       "  [0.2754341959953308,\n",
       "   0.24811534583568573,\n",
       "   0.24464547634124756,\n",
       "   0.23180502653121948],\n",
       "  [0.3317877948284149,\n",
       "   0.24854505062103271,\n",
       "   0.21164420247077942,\n",
       "   0.20802299678325653],\n",
       "  [0.33882808685302734,\n",
       "   0.25631412863731384,\n",
       "   0.22407397627830505,\n",
       "   0.18078380823135376],\n",
       "  [0.31276899576187134,\n",
       "   0.265206515789032,\n",
       "   0.2120400369167328,\n",
       "   0.20998446643352509],\n",
       "  [0.28019803762435913,\n",
       "   0.2749525010585785,\n",
       "   0.2689940333366394,\n",
       "   0.17585545778274536],\n",
       "  [0.30389633774757385,\n",
       "   0.2744991183280945,\n",
       "   0.2256917804479599,\n",
       "   0.19591279327869415],\n",
       "  [0.30576759576797485,\n",
       "   0.2741905450820923,\n",
       "   0.2478577047586441,\n",
       "   0.17218422889709473],\n",
       "  [0.30746087431907654,\n",
       "   0.28416717052459717,\n",
       "   0.24321721494197845,\n",
       "   0.16515472531318665],\n",
       "  [0.2942257821559906,\n",
       "   0.2729540169239044,\n",
       "   0.247698575258255,\n",
       "   0.18512165546417236],\n",
       "  [0.31512317061424255,\n",
       "   0.26496079564094543,\n",
       "   0.21014827489852905,\n",
       "   0.20976771414279938],\n",
       "  [0.31327584385871887,\n",
       "   0.30569350719451904,\n",
       "   0.1972881406545639,\n",
       "   0.18374258279800415],\n",
       "  [0.31681329011917114,\n",
       "   0.25794655084609985,\n",
       "   0.21924708783626556,\n",
       "   0.20599307119846344],\n",
       "  [0.3117213249206543,\n",
       "   0.26614904403686523,\n",
       "   0.2343960851430893,\n",
       "   0.1877334862947464],\n",
       "  [0.26446089148521423,\n",
       "   0.2626900374889374,\n",
       "   0.2508833706378937,\n",
       "   0.22196578979492188],\n",
       "  [0.273593544960022,\n",
       "   0.2729412317276001,\n",
       "   0.2522953450679779,\n",
       "   0.20116981863975525],\n",
       "  [0.28026169538497925,\n",
       "   0.27272653579711914,\n",
       "   0.23244595527648926,\n",
       "   0.21456585824489594],\n",
       "  [0.4296090304851532,\n",
       "   0.2302112728357315,\n",
       "   0.17290624976158142,\n",
       "   0.16727352142333984],\n",
       "  [0.301941841840744,\n",
       "   0.2709348499774933,\n",
       "   0.22460104525089264,\n",
       "   0.20252229273319244],\n",
       "  [0.287175714969635,\n",
       "   0.28375253081321716,\n",
       "   0.27901268005371094,\n",
       "   0.1500590741634369],\n",
       "  [0.30701878666877747,\n",
       "   0.29723256826400757,\n",
       "   0.2020919919013977,\n",
       "   0.19365671277046204],\n",
       "  [0.26930558681488037,\n",
       "   0.2591942846775055,\n",
       "   0.2522667646408081,\n",
       "   0.2192334234714508],\n",
       "  [0.3028859496116638,\n",
       "   0.2863396406173706,\n",
       "   0.23913666605949402,\n",
       "   0.17163780331611633],\n",
       "  [0.28834834694862366,\n",
       "   0.2615761458873749,\n",
       "   0.2536700367927551,\n",
       "   0.19640545547008514],\n",
       "  [0.31437817215919495,\n",
       "   0.26573675870895386,\n",
       "   0.2482725977897644,\n",
       "   0.17161248624324799],\n",
       "  [0.31492695212364197,\n",
       "   0.2662128210067749,\n",
       "   0.24799948930740356,\n",
       "   0.17086079716682434],\n",
       "  [0.30768662691116333,\n",
       "   0.305398553609848,\n",
       "   0.21470016241073608,\n",
       "   0.17221461236476898],\n",
       "  [0.3081015944480896,\n",
       "   0.26096588373184204,\n",
       "   0.2295929342508316,\n",
       "   0.20133954286575317],\n",
       "  [0.27911609411239624,\n",
       "   0.264859676361084,\n",
       "   0.2385128289461136,\n",
       "   0.21751144528388977],\n",
       "  [0.30286821722984314,\n",
       "   0.2850630581378937,\n",
       "   0.22216908633708954,\n",
       "   0.18989966809749603],\n",
       "  [0.31810465455055237,\n",
       "   0.2677215337753296,\n",
       "   0.2344021201133728,\n",
       "   0.17977161705493927],\n",
       "  [0.2955228090286255,\n",
       "   0.23569273948669434,\n",
       "   0.23481987416744232,\n",
       "   0.23396463692188263],\n",
       "  [0.29597190022468567,\n",
       "   0.26972338557243347,\n",
       "   0.24755769968032837,\n",
       "   0.18674704432487488],\n",
       "  [0.296487033367157,\n",
       "   0.29026880860328674,\n",
       "   0.22430717945098877,\n",
       "   0.18893691897392273],\n",
       "  [0.2732450067996979,\n",
       "   0.27313095331192017,\n",
       "   0.25470641255378723,\n",
       "   0.19891761243343353],\n",
       "  [0.3830559253692627,\n",
       "   0.3161323070526123,\n",
       "   0.1981288492679596,\n",
       "   0.10268298536539078],\n",
       "  [0.3054792582988739,\n",
       "   0.28774088621139526,\n",
       "   0.2288581281900406,\n",
       "   0.17792172729969025],\n",
       "  [0.4913537800312042,\n",
       "   0.20374701917171478,\n",
       "   0.15777108073234558,\n",
       "   0.14712809026241302],\n",
       "  [0.27375173568725586,\n",
       "   0.2664441466331482,\n",
       "   0.26198065280914307,\n",
       "   0.1978234052658081],\n",
       "  [0.30028286576271057,\n",
       "   0.27637529373168945,\n",
       "   0.22944322228431702,\n",
       "   0.19389860332012177],\n",
       "  [0.26491186022758484,\n",
       "   0.26132693886756897,\n",
       "   0.26112595200538635,\n",
       "   0.21263524889945984],\n",
       "  [0.38046786189079285,\n",
       "   0.35381168127059937,\n",
       "   0.13999195396900177,\n",
       "   0.12572850286960602],\n",
       "  [0.297893762588501,\n",
       "   0.26416417956352234,\n",
       "   0.22573119401931763,\n",
       "   0.21221087872982025],\n",
       "  [0.32597815990448,\n",
       "   0.247945636510849,\n",
       "   0.22837570309638977,\n",
       "   0.19770056009292603],\n",
       "  [0.40258458256721497,\n",
       "   0.2464972883462906,\n",
       "   0.18090277910232544,\n",
       "   0.1700153797864914],\n",
       "  [0.3144552707672119,\n",
       "   0.27663251757621765,\n",
       "   0.2327328622341156,\n",
       "   0.17617931962013245],\n",
       "  [0.4442010223865509,\n",
       "   0.30851811170578003,\n",
       "   0.15614236891269684,\n",
       "   0.09113851934671402],\n",
       "  [0.316087931394577,\n",
       "   0.29489225149154663,\n",
       "   0.2371641844511032,\n",
       "   0.15185566246509552],\n",
       "  [0.41393327713012695,\n",
       "   0.2477688044309616,\n",
       "   0.2173902690410614,\n",
       "   0.12090763449668884],\n",
       "  [0.3454488217830658,\n",
       "   0.31202733516693115,\n",
       "   0.22354340553283691,\n",
       "   0.11898043751716614],\n",
       "  [0.38546139001846313,\n",
       "   0.3337932825088501,\n",
       "   0.17824769020080566,\n",
       "   0.10249762237071991],\n",
       "  [0.4433143734931946,\n",
       "   0.3185867965221405,\n",
       "   0.1656389832496643,\n",
       "   0.07245978713035583],\n",
       "  [0.29000356793403625,\n",
       "   0.2653166353702545,\n",
       "   0.23475441336631775,\n",
       "   0.20992536842823029],\n",
       "  [0.2758502662181854,\n",
       "   0.25340375304222107,\n",
       "   0.2370341420173645,\n",
       "   0.23371189832687378],\n",
       "  [0.3995637893676758,\n",
       "   0.2760947048664093,\n",
       "   0.2534947395324707,\n",
       "   0.07084672152996063],\n",
       "  [0.26557520031929016,\n",
       "   0.25833603739738464,\n",
       "   0.24283236265182495,\n",
       "   0.23325638473033905],\n",
       "  [0.3588043451309204,\n",
       "   0.3298139274120331,\n",
       "   0.18804402649402618,\n",
       "   0.12333767116069794],\n",
       "  [0.2851962745189667,\n",
       "   0.2792447507381439,\n",
       "   0.24060025811195374,\n",
       "   0.1949586123228073],\n",
       "  [0.625063955783844,\n",
       "   0.1823275089263916,\n",
       "   0.1318521648645401,\n",
       "   0.06075633317232132],\n",
       "  [0.33305269479751587,\n",
       "   0.27610185742378235,\n",
       "   0.21375282108783722,\n",
       "   0.17709259688854218],\n",
       "  [0.29026055335998535,\n",
       "   0.27156922221183777,\n",
       "   0.269077867269516,\n",
       "   0.1690923571586609],\n",
       "  [0.39839619398117065,\n",
       "   0.2501005232334137,\n",
       "   0.22850140929222107,\n",
       "   0.12300192564725876],\n",
       "  [0.33442261815071106,\n",
       "   0.26231664419174194,\n",
       "   0.24254882335662842,\n",
       "   0.160711869597435],\n",
       "  [0.4231909215450287,\n",
       "   0.33197155594825745,\n",
       "   0.1878661960363388,\n",
       "   0.05697135254740715],\n",
       "  [0.36227908730506897,\n",
       "   0.2671087682247162,\n",
       "   0.22198368608951569,\n",
       "   0.14862850308418274],\n",
       "  [0.2998281419277191,\n",
       "   0.27149835228919983,\n",
       "   0.2239414006471634,\n",
       "   0.20473213493824005],\n",
       "  [0.3184712827205658,\n",
       "   0.2582726776599884,\n",
       "   0.24006971716880798,\n",
       "   0.183186337351799],\n",
       "  [0.33228111267089844,\n",
       "   0.23903325200080872,\n",
       "   0.23090487718582153,\n",
       "   0.1977807581424713],\n",
       "  [0.37220293283462524,\n",
       "   0.2481449991464615,\n",
       "   0.198403000831604,\n",
       "   0.18124905228614807],\n",
       "  [0.37642860412597656,\n",
       "   0.22868655622005463,\n",
       "   0.19946634769439697,\n",
       "   0.19541846215724945],\n",
       "  [0.4382381737232208,\n",
       "   0.22499437630176544,\n",
       "   0.1835976243019104,\n",
       "   0.15316975116729736],\n",
       "  [0.2704620361328125,\n",
       "   0.25780749320983887,\n",
       "   0.2517436444759369,\n",
       "   0.21998678147792816],\n",
       "  [0.344569593667984,\n",
       "   0.2573716938495636,\n",
       "   0.21583424508571625,\n",
       "   0.18222445249557495],\n",
       "  [0.31865471601486206,\n",
       "   0.24447114765644073,\n",
       "   0.22043754160404205,\n",
       "   0.21643659472465515],\n",
       "  [0.46016860008239746,\n",
       "   0.32490938901901245,\n",
       "   0.13248766958713531,\n",
       "   0.08243438601493835],\n",
       "  [0.43050166964530945,\n",
       "   0.2664324939250946,\n",
       "   0.23783111572265625,\n",
       "   0.06523468345403671],\n",
       "  [0.3241109251976013,\n",
       "   0.2666715979576111,\n",
       "   0.20619724690914154,\n",
       "   0.20302033424377441],\n",
       "  [0.33926689624786377,\n",
       "   0.26303553581237793,\n",
       "   0.24420426785945892,\n",
       "   0.15349330008029938],\n",
       "  [0.4289802014827728,\n",
       "   0.22187592089176178,\n",
       "   0.207145556807518,\n",
       "   0.1419982612133026],\n",
       "  [0.45366984605789185,\n",
       "   0.2086934596300125,\n",
       "   0.19883590936660767,\n",
       "   0.13880077004432678],\n",
       "  [0.32665181159973145,\n",
       "   0.2606786787509918,\n",
       "   0.2177906483411789,\n",
       "   0.194878950715065],\n",
       "  [0.3330150246620178,\n",
       "   0.2734380066394806,\n",
       "   0.24432437121868134,\n",
       "   0.14922258257865906],\n",
       "  [0.36270958185195923,\n",
       "   0.2725997567176819,\n",
       "   0.2236349880695343,\n",
       "   0.1410556137561798],\n",
       "  [0.37960395216941833,\n",
       "   0.26005998253822327,\n",
       "   0.18990501761436462,\n",
       "   0.17043106257915497],\n",
       "  [0.35701099038124084,\n",
       "   0.25210684537887573,\n",
       "   0.1979645937681198,\n",
       "   0.192917600274086],\n",
       "  [0.5009943246841431,\n",
       "   0.22890561819076538,\n",
       "   0.1384906768798828,\n",
       "   0.13160938024520874],\n",
       "  [0.34530264139175415,\n",
       "   0.29663676023483276,\n",
       "   0.2359326034784317,\n",
       "   0.12212797999382019],\n",
       "  [0.3118043541908264,\n",
       "   0.29091671109199524,\n",
       "   0.20182789862155914,\n",
       "   0.1954510360956192],\n",
       "  [0.29869234561920166,\n",
       "   0.2587242126464844,\n",
       "   0.23381781578063965,\n",
       "   0.20876556634902954],\n",
       "  [0.2697712779045105,\n",
       "   0.2516689896583557,\n",
       "   0.2406342625617981,\n",
       "   0.23792551457881927],\n",
       "  [0.9158260226249695,\n",
       "   0.04051937535405159,\n",
       "   0.02195461466908455,\n",
       "   0.02170003578066826],\n",
       "  [0.363633394241333,\n",
       "   0.23650124669075012,\n",
       "   0.22025655210018158,\n",
       "   0.1796088069677353],\n",
       "  [0.30875155329704285,\n",
       "   0.29900720715522766,\n",
       "   0.19721612334251404,\n",
       "   0.19502511620521545],\n",
       "  [0.2713479697704315,\n",
       "   0.2629145383834839,\n",
       "   0.23513300716876984,\n",
       "   0.23060442507266998],\n",
       "  [0.3656696379184723,\n",
       "   0.2522883415222168,\n",
       "   0.2469996213912964,\n",
       "   0.13504239916801453],\n",
       "  [0.30956724286079407,\n",
       "   0.26567888259887695,\n",
       "   0.2205248773097992,\n",
       "   0.2042289525270462],\n",
       "  [0.27453505992889404,\n",
       "   0.27062276005744934,\n",
       "   0.2535357177257538,\n",
       "   0.20130649209022522],\n",
       "  [0.3098488450050354,\n",
       "   0.24500592052936554,\n",
       "   0.23644821345806122,\n",
       "   0.20869700610637665],\n",
       "  [0.26272913813591003,\n",
       "   0.25432389974594116,\n",
       "   0.24968519806861877,\n",
       "   0.23326176404953003],\n",
       "  [0.38136881589889526,\n",
       "   0.22579500079154968,\n",
       "   0.2077680379152298,\n",
       "   0.18506816029548645],\n",
       "  [0.31817805767059326,\n",
       "   0.2927277088165283,\n",
       "   0.27176985144615173,\n",
       "   0.11732438951730728],\n",
       "  [0.39067599177360535,\n",
       "   0.25575366616249084,\n",
       "   0.18800021708011627,\n",
       "   0.16557009518146515],\n",
       "  [0.5147355794906616,\n",
       "   0.3010839819908142,\n",
       "   0.10242610424757004,\n",
       "   0.08175427466630936],\n",
       "  [0.3051944673061371,\n",
       "   0.26370206475257874,\n",
       "   0.23648536205291748,\n",
       "   0.1946181207895279],\n",
       "  [0.431119829416275,\n",
       "   0.23057983815670013,\n",
       "   0.20498454570770264,\n",
       "   0.13331574201583862],\n",
       "  [0.3448594808578491,\n",
       "   0.27048343420028687,\n",
       "   0.23947302997112274,\n",
       "   0.14518402516841888],\n",
       "  [0.29421287775039673,\n",
       "   0.27964797616004944,\n",
       "   0.22443923354148865,\n",
       "   0.20169992744922638],\n",
       "  [0.37917494773864746,\n",
       "   0.23523323237895966,\n",
       "   0.2184532880783081,\n",
       "   0.16713851690292358],\n",
       "  [0.3514569103717804,\n",
       "   0.27995550632476807,\n",
       "   0.19611652195453644,\n",
       "   0.17247110605239868],\n",
       "  [0.3106652796268463,\n",
       "   0.2586281895637512,\n",
       "   0.24533617496490479,\n",
       "   0.18537037074565887],\n",
       "  [0.4899275004863739,\n",
       "   0.21616114675998688,\n",
       "   0.21282516419887543,\n",
       "   0.0810861587524414],\n",
       "  [0.5054928660392761,\n",
       "   0.21548917889595032,\n",
       "   0.17942644655704498,\n",
       "   0.09959147125482559],\n",
       "  [0.4903661608695984,\n",
       "   0.19169574975967407,\n",
       "   0.16326285898685455,\n",
       "   0.15467523038387299],\n",
       "  [0.4584222733974457,\n",
       "   0.23194175958633423,\n",
       "   0.23093968629837036,\n",
       "   0.07869624346494675],\n",
       "  [0.29240840673446655,\n",
       "   0.2917839288711548,\n",
       "   0.21330289542675018,\n",
       "   0.20250476896762848],\n",
       "  [0.42106860876083374,\n",
       "   0.21683911979198456,\n",
       "   0.19433605670928955,\n",
       "   0.16775621473789215],\n",
       "  [0.28426238894462585,\n",
       "   0.26249921321868896,\n",
       "   0.22923947870731354,\n",
       "   0.22399893403053284],\n",
       "  [0.2829870879650116,\n",
       "   0.277681440114975,\n",
       "   0.23612108826637268,\n",
       "   0.20321039855480194],\n",
       "  [0.2782497704029083,\n",
       "   0.27373310923576355,\n",
       "   0.26449769735336304,\n",
       "   0.1835194081068039],\n",
       "  [0.5499168634414673,\n",
       "   0.1672995537519455,\n",
       "   0.16187231242656708,\n",
       "   0.12091131508350372],\n",
       "  [0.2828594446182251,\n",
       "   0.28230172395706177,\n",
       "   0.2717762589454651,\n",
       "   0.16306264698505402],\n",
       "  [0.3015233874320984,\n",
       "   0.2998659610748291,\n",
       "   0.21475821733474731,\n",
       "   0.1838524490594864],\n",
       "  [0.8722280859947205,\n",
       "   0.04857228696346283,\n",
       "   0.04322206228971481,\n",
       "   0.03597753122448921],\n",
       "  [0.3325803875923157,\n",
       "   0.23254695534706116,\n",
       "   0.21787384152412415,\n",
       "   0.2169988751411438],\n",
       "  [0.3382736146450043,\n",
       "   0.2648586928844452,\n",
       "   0.25730377435684204,\n",
       "   0.1395639330148697],\n",
       "  [0.3312559425830841,\n",
       "   0.2471778392791748,\n",
       "   0.22046303749084473,\n",
       "   0.20110321044921875],\n",
       "  [0.5056778788566589,\n",
       "   0.22282907366752625,\n",
       "   0.1918640434741974,\n",
       "   0.07962896674871445],\n",
       "  [0.5607938766479492,\n",
       "   0.33434006571769714,\n",
       "   0.07578457146883011,\n",
       "   0.029081489890813828],\n",
       "  [0.3266680836677551,\n",
       "   0.2718925178050995,\n",
       "   0.2163841426372528,\n",
       "   0.1850551962852478],\n",
       "  [0.3349502980709076,\n",
       "   0.2556556463241577,\n",
       "   0.24136680364608765,\n",
       "   0.16802719235420227],\n",
       "  [0.35604414343833923,\n",
       "   0.33850806951522827,\n",
       "   0.18343037366867065,\n",
       "   0.12201748043298721],\n",
       "  [0.41101008653640747,\n",
       "   0.2575051188468933,\n",
       "   0.1719428449869156,\n",
       "   0.15954191982746124],\n",
       "  [0.27325835824012756,\n",
       "   0.27156996726989746,\n",
       "   0.2404140681028366,\n",
       "   0.2147575467824936],\n",
       "  [0.5865312814712524,\n",
       "   0.19216248393058777,\n",
       "   0.1249588280916214,\n",
       "   0.09634742140769958],\n",
       "  [0.3840159475803375,\n",
       "   0.25414735078811646,\n",
       "   0.2206241935491562,\n",
       "   0.14121246337890625],\n",
       "  [0.2712820768356323,\n",
       "   0.25769442319869995,\n",
       "   0.2359710931777954,\n",
       "   0.23505248129367828],\n",
       "  [0.4802348017692566,\n",
       "   0.34367480874061584,\n",
       "   0.09902063012123108,\n",
       "   0.07706977427005768],\n",
       "  [0.28004804253578186,\n",
       "   0.25374335050582886,\n",
       "   0.23838864266872406,\n",
       "   0.22781988978385925],\n",
       "  [0.31571945548057556,\n",
       "   0.30389395356178284,\n",
       "   0.1993371546268463,\n",
       "   0.18104946613311768],\n",
       "  [0.3262941539287567,\n",
       "   0.28475692868232727,\n",
       "   0.26677924394607544,\n",
       "   0.12216971069574356],\n",
       "  [0.3128674328327179,\n",
       "   0.29192739725112915,\n",
       "   0.20892450213432312,\n",
       "   0.18628066778182983],\n",
       "  [0.2902153432369232,\n",
       "   0.26419493556022644,\n",
       "   0.25204789638519287,\n",
       "   0.1935417652130127],\n",
       "  [0.2757699489593506,\n",
       "   0.2694884240627289,\n",
       "   0.25763943791389465,\n",
       "   0.1971021294593811],\n",
       "  [0.3641147017478943,\n",
       "   0.3368123471736908,\n",
       "   0.17271794378757477,\n",
       "   0.12635503709316254],\n",
       "  [0.29187360405921936,\n",
       "   0.25997060537338257,\n",
       "   0.23093649744987488,\n",
       "   0.21721936762332916],\n",
       "  [0.3591407537460327,\n",
       "   0.2740092873573303,\n",
       "   0.20527280867099762,\n",
       "   0.16157719492912292],\n",
       "  [0.33897536993026733,\n",
       "   0.276629239320755,\n",
       "   0.2318267524242401,\n",
       "   0.152568519115448],\n",
       "  [0.2771227955818176,\n",
       "   0.25110533833503723,\n",
       "   0.23804470896720886,\n",
       "   0.23372721672058105],\n",
       "  [0.2948577404022217,\n",
       "   0.2822641134262085,\n",
       "   0.2305244356393814,\n",
       "   0.19235362112522125],\n",
       "  [0.3353852927684784,\n",
       "   0.3234991729259491,\n",
       "   0.25169870257377625,\n",
       "   0.08941682428121567],\n",
       "  [0.45480963587760925,\n",
       "   0.2974020838737488,\n",
       "   0.14406046271324158,\n",
       "   0.1037277951836586],\n",
       "  [0.43347951769828796,\n",
       "   0.2954005300998688,\n",
       "   0.17152780294418335,\n",
       "   0.09959214180707932],\n",
       "  [0.31251832842826843,\n",
       "   0.2938218116760254,\n",
       "   0.2184474617242813,\n",
       "   0.17521238327026367],\n",
       "  [0.43253201246261597,\n",
       "   0.23137614130973816,\n",
       "   0.19659177958965302,\n",
       "   0.13950006663799286],\n",
       "  [0.3183113932609558,\n",
       "   0.3049531579017639,\n",
       "   0.21392273902893066,\n",
       "   0.162812739610672],\n",
       "  [0.3186815679073334,\n",
       "   0.24849724769592285,\n",
       "   0.2398533970117569,\n",
       "   0.1929677575826645],\n",
       "  [0.44197797775268555,\n",
       "   0.36118724942207336,\n",
       "   0.13504552841186523,\n",
       "   0.06178924813866615],\n",
       "  [0.3611315190792084,\n",
       "   0.2777015268802643,\n",
       "   0.24870270490646362,\n",
       "   0.11246420443058014],\n",
       "  [0.2712181806564331,\n",
       "   0.2705141305923462,\n",
       "   0.25285589694976807,\n",
       "   0.20541180670261383],\n",
       "  [0.6472867727279663,\n",
       "   0.17595767974853516,\n",
       "   0.11545953899621964,\n",
       "   0.061295993626117706],\n",
       "  [0.39207902550697327,\n",
       "   0.33118966221809387,\n",
       "   0.14804813265800476,\n",
       "   0.1286831796169281],\n",
       "  [0.4387679696083069,\n",
       "   0.24855081737041473,\n",
       "   0.15726591646671295,\n",
       "   0.155415341258049],\n",
       "  [0.3039490580558777,\n",
       "   0.2910640835762024,\n",
       "   0.26546964049339294,\n",
       "   0.13951727747917175],\n",
       "  [0.33341774344444275,\n",
       "   0.2698264718055725,\n",
       "   0.20322562754154205,\n",
       "   0.1935301572084427],\n",
       "  [0.3005504310131073,\n",
       "   0.2549798786640167,\n",
       "   0.24649451673030853,\n",
       "   0.19797514379024506],\n",
       "  [0.4749441146850586,\n",
       "   0.21017497777938843,\n",
       "   0.17405454814434052,\n",
       "   0.14082638919353485],\n",
       "  [0.3955968916416168,\n",
       "   0.21881146728992462,\n",
       "   0.21591784060001373,\n",
       "   0.16967378556728363],\n",
       "  [0.3484743535518646,\n",
       "   0.22724978625774384,\n",
       "   0.2259572595357895,\n",
       "   0.19831866025924683],\n",
       "  [0.3145884573459625,\n",
       "   0.26071998476982117,\n",
       "   0.23552431166172028,\n",
       "   0.18916717171669006],\n",
       "  [0.31382572650909424,\n",
       "   0.2512250244617462,\n",
       "   0.24242404103279114,\n",
       "   0.1925252079963684],\n",
       "  [0.3885866105556488,\n",
       "   0.23523499071598053,\n",
       "   0.2131318897008896,\n",
       "   0.16304652392864227],\n",
       "  [0.3185201585292816,\n",
       "   0.31422311067581177,\n",
       "   0.21815142035484314,\n",
       "   0.14910531044006348],\n",
       "  [0.3352661430835724,\n",
       "   0.24159300327301025,\n",
       "   0.21224407851696014,\n",
       "   0.21089671552181244],\n",
       "  [0.3042205572128296,\n",
       "   0.2805892527103424,\n",
       "   0.25513485074043274,\n",
       "   0.16005532443523407],\n",
       "  [0.28333425521850586,\n",
       "   0.26749011874198914,\n",
       "   0.25540778040885925,\n",
       "   0.19376786053180695],\n",
       "  [0.3608526587486267,\n",
       "   0.2696996331214905,\n",
       "   0.19141869246959686,\n",
       "   0.17802909016609192],\n",
       "  [0.29472193121910095,\n",
       "   0.2396349459886551,\n",
       "   0.23957622051239014,\n",
       "   0.2260669469833374],\n",
       "  [0.3878919184207916,\n",
       "   0.29532405734062195,\n",
       "   0.16029201447963715,\n",
       "   0.1564919799566269],\n",
       "  [0.3249841034412384,\n",
       "   0.26006585359573364,\n",
       "   0.24772413074970245,\n",
       "   0.1672259271144867],\n",
       "  [0.33832496404647827,\n",
       "   0.2429480105638504,\n",
       "   0.2344307005405426,\n",
       "   0.18429626524448395],\n",
       "  [0.42612960934638977,\n",
       "   0.27333298325538635,\n",
       "   0.1605178415775299,\n",
       "   0.14001961052417755],\n",
       "  [0.43427520990371704,\n",
       "   0.26640942692756653,\n",
       "   0.17795732617378235,\n",
       "   0.12135796248912811],\n",
       "  [0.422921746969223,\n",
       "   0.26792827248573303,\n",
       "   0.15943606197834015,\n",
       "   0.14971394836902618],\n",
       "  [0.26803967356681824,\n",
       "   0.2672983407974243,\n",
       "   0.2540666162967682,\n",
       "   0.21059533953666687],\n",
       "  [0.3871839642524719,\n",
       "   0.2416577786207199,\n",
       "   0.20294323563575745,\n",
       "   0.1682150512933731],\n",
       "  [0.28397950530052185,\n",
       "   0.2800735831260681,\n",
       "   0.2311026006937027,\n",
       "   0.2048443704843521],\n",
       "  [0.5183571577072144,\n",
       "   0.27015161514282227,\n",
       "   0.11269503831863403,\n",
       "   0.09879624098539352],\n",
       "  [0.3227795660495758,\n",
       "   0.2935422956943512,\n",
       "   0.20346881449222565,\n",
       "   0.18020939826965332],\n",
       "  [0.36289897561073303,\n",
       "   0.3072914481163025,\n",
       "   0.16825641691684723,\n",
       "   0.16155321896076202],\n",
       "  [0.3336346745491028,\n",
       "   0.3075599670410156,\n",
       "   0.21732176840305328,\n",
       "   0.1414836198091507],\n",
       "  [0.29586175084114075,\n",
       "   0.2909645736217499,\n",
       "   0.28491541743278503,\n",
       "   0.12825822830200195],\n",
       "  [0.39894628524780273,\n",
       "   0.2583322525024414,\n",
       "   0.1895758956670761,\n",
       "   0.15314555168151855],\n",
       "  [0.29930976033210754,\n",
       "   0.2562752664089203,\n",
       "   0.23526200652122498,\n",
       "   0.2091529369354248],\n",
       "  [0.3175044655799866,\n",
       "   0.24691148102283478,\n",
       "   0.22881685197353363,\n",
       "   0.20676720142364502],\n",
       "  [0.33624979853630066,\n",
       "   0.24656303226947784,\n",
       "   0.22017742693424225,\n",
       "   0.19700968265533447],\n",
       "  [0.3358711898326874,\n",
       "   0.2661486566066742,\n",
       "   0.24639591574668884,\n",
       "   0.1515842080116272],\n",
       "  [0.2920539379119873,\n",
       "   0.29193055629730225,\n",
       "   0.24183832108974457,\n",
       "   0.1741771101951599],\n",
       "  [0.35534289479255676,\n",
       "   0.29247233271598816,\n",
       "   0.21284300088882446,\n",
       "   0.13934169709682465],\n",
       "  [0.29350200295448303,\n",
       "   0.2574544847011566,\n",
       "   0.25237736105918884,\n",
       "   0.1966661661863327],\n",
       "  [0.4030519127845764,\n",
       "   0.28024712204933167,\n",
       "   0.20050114393234253,\n",
       "   0.11619985103607178]],\n",
       " 'top_inds': [[3, 1, 2, 0],\n",
       "  [1, 2, 0, 3],\n",
       "  [0, 2, 3, 1],\n",
       "  [1, 2, 0, 3],\n",
       "  [1, 0, 3, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [1, 3, 2, 0],\n",
       "  [2, 1, 3, 0],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [1, 3, 0, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [3, 2, 1, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [2, 0, 3, 1],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [1, 0, 2, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [2, 0, 1, 3],\n",
       "  [1, 3, 0, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 0, 1, 2],\n",
       "  [0, 1, 2, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [2, 1, 0, 3],\n",
       "  [3, 1, 2, 0],\n",
       "  [3, 2, 1, 0],\n",
       "  [0, 1, 2, 3],\n",
       "  [3, 0, 1, 2],\n",
       "  [0, 2, 1, 3],\n",
       "  [1, 0, 2, 3],\n",
       "  [1, 2, 3, 0],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 3, 0],\n",
       "  [0, 2, 1, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [2, 1, 3, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 0, 3, 1],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 1, 2, 0],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [1, 2, 3, 0],\n",
       "  [0, 2, 1, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 3, 1, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 2, 1, 0],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 3, 0],\n",
       "  [0, 2, 1, 3],\n",
       "  [0, 1, 3, 2],\n",
       "  [0, 2, 1, 3],\n",
       "  [3, 2, 1, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [2, 3, 0, 1],\n",
       "  [1, 2, 3, 0],\n",
       "  [1, 2, 3, 0],\n",
       "  [2, 1, 0, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [3, 2, 1, 0],\n",
       "  [0, 2, 1, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 2, 3, 1],\n",
       "  [3, 0, 2, 1],\n",
       "  [0, 3, 2, 1],\n",
       "  [2, 0, 1, 3],\n",
       "  [0, 3, 1, 2],\n",
       "  [1, 3, 0, 2],\n",
       "  [0, 1, 3, 2],\n",
       "  [3, 1, 2, 0],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 2, 3, 1],\n",
       "  [1, 3, 0, 2],\n",
       "  [3, 1, 0, 2],\n",
       "  [3, 2, 1, 0],\n",
       "  [1, 3, 2, 0],\n",
       "  [0, 1, 2, 3],\n",
       "  [1, 0, 2, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 3, 1, 0],\n",
       "  [2, 3, 1, 0],\n",
       "  [0, 3, 1, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [1, 0, 3, 2],\n",
       "  [3, 1, 2, 0],\n",
       "  [2, 0, 3, 1],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 1, 3, 2],\n",
       "  [0, 3, 2, 1],\n",
       "  [2, 0, 3, 1],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 3, 0, 1],\n",
       "  [2, 0, 1, 3],\n",
       "  [0, 2, 1, 3],\n",
       "  [2, 0, 3, 1],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 3, 1, 2],\n",
       "  [0, 3, 2, 1],\n",
       "  [3, 1, 0, 2],\n",
       "  [3, 2, 0, 1],\n",
       "  [3, 0, 2, 1],\n",
       "  [2, 1, 0, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [1, 0, 3, 2],\n",
       "  [1, 2, 0, 3],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [1, 3, 2, 0],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 3, 1, 2],\n",
       "  [0, 2, 3, 1],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 3, 0, 1],\n",
       "  [1, 2, 3, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [0, 1, 2, 3],\n",
       "  [3, 0, 2, 1],\n",
       "  [0, 1, 2, 3],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [2, 1, 3, 0],\n",
       "  [3, 0, 2, 1],\n",
       "  [1, 2, 3, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [1, 2, 3, 0],\n",
       "  [0, 2, 1, 3],\n",
       "  [3, 2, 1, 0],\n",
       "  [3, 0, 1, 2],\n",
       "  [0, 1, 3, 2],\n",
       "  [1, 3, 2, 0],\n",
       "  [0, 2, 3, 1],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 1, 3, 2],\n",
       "  [3, 2, 0, 1],\n",
       "  [1, 0, 2, 3],\n",
       "  [2, 0, 3, 1],\n",
       "  [2, 1, 3, 0],\n",
       "  [3, 0, 1, 2],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 0, 1, 3],\n",
       "  [1, 3, 2, 0],\n",
       "  [0, 2, 3, 1],\n",
       "  [2, 3, 1, 0],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 1, 3, 0],\n",
       "  [3, 2, 1, 0],\n",
       "  [3, 2, 1, 0],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 1, 2, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [0, 3, 1, 2],\n",
       "  [2, 3, 0, 1],\n",
       "  [1, 0, 3, 2],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [1, 3, 2, 0],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 1, 2, 3],\n",
       "  [2, 1, 0, 3],\n",
       "  [2, 0, 3, 1],\n",
       "  [0, 1, 2, 3],\n",
       "  [2, 3, 1, 0],\n",
       "  [0, 1, 2, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [1, 2, 3, 0],\n",
       "  [2, 3, 0, 1],\n",
       "  [1, 2, 0, 3],\n",
       "  [2, 0, 3, 1],\n",
       "  [0, 1, 3, 2],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 3, 1, 2],\n",
       "  [2, 3, 0, 1],\n",
       "  [3, 2, 1, 0],\n",
       "  [3, 1, 0, 2],\n",
       "  [2, 1, 3, 0],\n",
       "  [1, 2, 3, 0],\n",
       "  [2, 0, 1, 3],\n",
       "  [3, 1, 0, 2],\n",
       "  [1, 3, 2, 0],\n",
       "  [2, 3, 0, 1],\n",
       "  [0, 1, 3, 2],\n",
       "  [1, 2, 0, 3],\n",
       "  [3, 0, 2, 1],\n",
       "  [2, 1, 0, 3],\n",
       "  [0, 2, 3, 1],\n",
       "  [1, 3, 2, 0],\n",
       "  [3, 1, 2, 0],\n",
       "  [2, 1, 3, 0],\n",
       "  [0, 2, 3, 1],\n",
       "  [2, 3, 0, 1],\n",
       "  [2, 1, 3, 0],\n",
       "  [1, 3, 0, 2],\n",
       "  [3, 1, 0, 2],\n",
       "  [3, 1, 2, 0],\n",
       "  [0, 2, 3, 1],\n",
       "  [3, 2, 1, 0],\n",
       "  [1, 0, 2, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [1, 0, 3, 2],\n",
       "  [2, 3, 1, 0],\n",
       "  [1, 0, 2, 3],\n",
       "  [0, 2, 3, 1],\n",
       "  [1, 3, 2, 0],\n",
       "  [1, 3, 2, 0],\n",
       "  [3, 1, 0, 2],\n",
       "  [3, 0, 2, 1],\n",
       "  [1, 2, 0, 3],\n",
       "  [1, 2, 0, 3],\n",
       "  [3, 2, 0, 1],\n",
       "  [3, 0, 1, 2],\n",
       "  [1, 0, 2, 3],\n",
       "  [1, 3, 0, 2],\n",
       "  [2, 1, 0, 3],\n",
       "  [1, 0, 2, 3],\n",
       "  [0, 1, 2, 3],\n",
       "  [0, 3, 1, 2],\n",
       "  [3, 2, 1, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = naive_zero_shot_classifier_pipeline(\n",
    "    zero_shot_classifier, ds.take(256)\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfG0cp3RNwLG"
   },
   "source": [
    "#### Batched implementation\n",
    "Rewrite the pipeline to process the dataset in batches to improve efficiency.  \n",
    "ModernBERT supports a special batching mode called *sequence packing* but its usage requires FlashAttention and is beyond the scope of this task.  \n",
    "Your goal is to implement batching in such a way that the processing of the whole dataset is fast, gpu utilization is high and you don't run out of memory.\n",
    "\n",
    "**Hint (general):** group inputs in some specific way to minimize the amount of padding tokens.   \n",
    "**Hint (implementation):** You may (but don't have to) check the implementation of the \"zero-shot-classification\" pipeline in Hugging Face transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iBFtqBgPNwLG"
   },
   "outputs": [],
   "source": [
    "def zero_shot_classifier_with_batching(zero_shot_classifier, pipeline_input) -> PipelineResult:\n",
    "    pipeline_input = list(pipeline_input)\n",
    "    batch_size = 8  \n",
    "\n",
    "    all_labels, all_scores, all_top_inds = [], [], []\n",
    "\n",
    "    for i in range(0, len(pipeline_input), batch_size):\n",
    "        batch = pipeline_input[i:i + batch_size]\n",
    "\n",
    "        for sample in batch:\n",
    "            question = sample[\"question\"]\n",
    "            options = sample[\"options\"]\n",
    "\n",
    "            result = zero_shot_classifier(\n",
    "                question,\n",
    "                options,\n",
    "                hypothesis_template=\"The correct answer is: {}\",\n",
    "                multi_label=False,\n",
    "            )\n",
    "\n",
    "            all_labels.append(result[\"labels\"])\n",
    "            all_scores.append(result[\"scores\"])\n",
    "            all_top_inds.append([options.index(label) for label in result[\"labels\"]])\n",
    "\n",
    "    return {\n",
    "        \"labels\": all_labels,\n",
    "        \"scores\": all_scores,\n",
    "        \"top_inds\": all_top_inds,\n",
    "    }\n",
    "\n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "flush = sys.stdout.flush\n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Avxo0FujNwLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 11.4 s, total: 2min 1s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r_batched = zero_shot_classifier_with_batching(\n",
    "    zero_shot_classifier, ds.take(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q166omDbNwLG"
   },
   "source": [
    "#### Test naive vs batched\n",
    "Write a test checking that naive and vectorized implementations produce same results.\n",
    "\n",
    "**Hint**: there might be some examples in the data which break the comparison.  \n",
    "You may remove them or adjust the function to handle them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jT1wOCIcNwLG"
   },
   "outputs": [],
   "source": [
    "def compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, data: Dataset):\n",
    "    data = list(data) \n",
    "\n",
    "    print(\"Running naive implementation...\")\n",
    "    result_naive = naive_zero_shot_classifier_pipeline(zero_shot_classifier, data)\n",
    "\n",
    "    print(\"Running batched implementation...\")\n",
    "    result_batched = zero_shot_classifier_with_batching(zero_shot_classifier, data)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        naive_labels = result_naive[\"labels\"][i]\n",
    "        batched_labels = result_batched[\"labels\"][i]\n",
    "\n",
    "        if naive_labels != batched_labels:\n",
    "            print(f\" Mismatch at index {i}\")\n",
    "            print(\"Naive:\", naive_labels)\n",
    "            print(\"Batched:\", batched_labels)\n",
    "            return\n",
    "\n",
    "    print(\" Naive and batched outputs match exactly.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PoGkyvrqNwLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running naive implementation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [10:36<00:00,  2.49s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched implementation...\n",
      " Naive and batched outputs match exactly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_naive_and_bathched_zero_shot_classifiers(zero_shot_classifier, ds.shuffle(42).take(256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFKWwMd4NwLG"
   },
   "source": [
    "#### Profiling\n",
    "Profile both implementations with Torch profiler.  \n",
    "Include the results as screenhots and comment on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling naive version...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:17<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 1.58 s, total: 17.1 s\n",
      "Wall time: 17.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling batched version...\n",
      "CPU times: user 15.4 s, sys: 1.14 s, total: 16.5 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "flush = sys.stdout.flush\n",
    "\n",
    "print(\"Profiling naive version...\")\n",
    "%time r_naive = naive_zero_shot_classifier_pipeline(zero_shot_classifier, ds.select(range(32)))\n",
    "flush()\n",
    "\n",
    "print(\"Profiling batched version...\")\n",
    "%time r_batched = zero_shot_classifier_with_batching(zero_shot_classifier, ds.select(range(32)))\n",
    "flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vJzkKb1NwLG"
   },
   "source": [
    "### ✅ Profiling Results & Batch Size Selection (Task 4)\n",
    "\n",
    "We profiled both versions of the pipeline using 32 examples:\n",
    "\n",
    "- **Naive implementation:**\n",
    "  - Wall time: **17.4 seconds**\n",
    "  - Speed: ~1.84 examples per second\n",
    "\n",
    "- **Batched implementation:**\n",
    "  - Wall time: **16.6 seconds**\n",
    "  - Speed: ~1.93 examples per second\n",
    "\n",
    "Although the gain is small for small datasets, batching significantly improves throughput when processing larger datasets. We selected `batch_size = 8` for all further experiments because:\n",
    "- It avoids memory errors on MPS backend (Apple Silicon)\n",
    "- It maintains good performance while being memory-efficient\n",
    "\n",
    "Sequence packing or FlashAttention was not used, as it is beyond the scope of this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w2XXhCfNwLG"
   },
   "source": [
    "### Process the whole dataset & calculate metrics\n",
    "Here you should process the whole dataset.  \n",
    "Note the time it took.  \n",
    "Then calculate some metrics (accuracy and other you may like) and comment on them.  \n",
    "If you don't have the batched implementation, you may process the dataset with the naive version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(result: PipelineResult, dataset):\n",
    "    correct = 0\n",
    "    for i in range(len(dataset)):\n",
    "        correct_answer = [\"a\", \"b\", \"c\", \"d\"].index(dataset[i][\"answer\"].lower())\n",
    "        if result[\"top_inds\"][i][0] == correct_answer:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(dataset)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lHIlvgCdNwLG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.55%\n",
      "CPU times: user 2min 17s, sys: 10.3 s, total: 2min 27s\n",
      "Wall time: 2min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35546875"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subset = ds.select(range(256))\n",
    "r_small = zero_shot_classifier_with_batching(zero_shot_classifier, subset)\n",
    "evaluate_accuracy(r_small, subset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2xeH2WqNwLG"
   },
   "source": [
    "### Part 3: Decoder Models  \n",
    "\n",
    "In this section, we will explore how to adapt a decoder model to solve this task and how modern LLMs are benchmarked on it.  \n",
    "\n",
    "Recall that decoder models are used for autoregressive text generation, meaning they predict one token at a time, conditioning each prediction on previously generated tokens. A natural way to solve this task would be to prompt the model with different answer options and let it generate a response. However, this approach presents two major challenges:  \n",
    "\n",
    "1. The model may not generate the answer in the expected format, making automatic evaluation difficult.  \n",
    "2. Since decoder models generate text step by step, they do not directly assign a single probability to an entire answer, making it hard to compare different answer choices.  \n",
    "\n",
    "To address this, we use **perplexity** to evaluate how likely the model considers each possible answer.  \n",
    "\n",
    "### Perplexity-Based Evaluation  \n",
    "\n",
    "Since a decoder model predicts a probability distribution over the vocabulary for each token, we can compute the likelihood of any given sequence by multiplying the probabilities assigned to its tokens. Perplexity is a measure of how well the model predicts a sequence, defined as the exponentiated negative average log-likelihood of the sequence. Formally, for a sequence of tokens $\\mathbf{w} = (w_1, w_2, ..., w_n)$, perplexity is computed as:  \n",
    "\n",
    "$\n",
    "PPL(\\mathbf{w}) = \\exp \\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\log P(w_i \\mid w_{<i}) \\right)\n",
    "$\n",
    "\n",
    "where $ P(w_i \\mid w_{<i}) $ is the probability assigned by the model to token $ w_i $ given the preceding tokens.  \n",
    "\n",
    "A lower perplexity score indicates that the model assigns a higher probability to the given answer, making it a more likely choice. By computing perplexity for each possible answer and selecting the one with the lowest value, we can systematically rank the answers without requiring the model to generate them explicitly.  \n",
    "\n",
    "This approach ensures reliable and scalable evaluation, making it a standard technique for benchmarking decoder models on multiple-choice tasks.  \n",
    "\n",
    "You can read more about perplexity and what problems there are when it comes to using it as a metric in [this short blog](https://blog.eleuther.ai/multiple-choice-normalization/). Notice the challenges when it comes to models with different tokenizers and how to overcome them.\n",
    "\n",
    "Last but not least there is reproducibility issue if you deploy big optimized model on moder GPU, you can read more about it [here](https://community.openai.com/t/a-question-on-determinism/8185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "w6JWcr--NwLH"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6xSOdN7NwLH"
   },
   "source": [
    "#### Revisiting the prompt\n",
    "\n",
    "The prompt and response format also matters. You can read more about that [here](https://huggingface.co/blog/open-llm-leaderboard-mmlu) and also about the differences when it comes to deciding which answer model choosed. You can read in this blog that depending on the prompt and evaluation strategy the benchmark results can vary.\n",
    "\n",
    "We will use HELM prompt and normalize perplexity by token its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kdY4AK5MNwLH",
    "outputId": "3e63bc7c-07ce-45d4-a589-b5faee040a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are multiple choice questions (with answers) about abstract_algebra:\n",
      "\n",
      "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\n",
      "A. 0\n",
      "B. 4\n",
      "C. 2\n",
      "D. 6\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "HELM_PROMPT_TEMPLATE = dedent(\"\"\"\n",
    "The following are multiple choice questions (with answers) about {subject}:\n",
    "\n",
    "{question}\n",
    "A. {option_a}\n",
    "B. {option_b}\n",
    "C. {option_c}\n",
    "D. {option_d}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "print(\n",
    "    HELM_PROMPT_TEMPLATE.format(\n",
    "        subject=sample_subject,\n",
    "        question=sample_question,\n",
    "        option_a=options[0],\n",
    "        option_b=options[1],\n",
    "        option_c=options[2],\n",
    "        option_d=options[3],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv5qgSgjNwLH"
   },
   "source": [
    "Let's generate sample answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jMUn5OytNwLH",
    "outputId": "40cd1dbe-6914-4790-a46e-25501666270a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: \n",
      "Q(sqrt(2), q(2), q(0)), w(2), w(10), 0\n",
      "\n",
      "Answer\n",
      "\n",
      "\n",
      "Attempt 2: \n",
      "A. 0, 3, 17, 27, 40, 50\n",
      "\n",
      "(q = r2(sqrt(2), r2((\n",
      "Attempt 3: \n",
      "This might be a problem for a first-order algebraic system because these fields have no particular choice (in this case, an optional field).\n",
      "Attempt 4: \n",
      "A. (sqrt(2), sqrt(3), sqrt(28)) would take (sqrt(3); q^2\n",
      "Attempt 5: \n",
      "A = *(0-4)*3 - 3\n",
      "\n",
      "B = *(0-5)*2 - 3\n",
      "\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "sample_prompt_formatted = HELM_PROMPT_TEMPLATE.format(\n",
    "    subject=sample_subject,\n",
    "    question=sample_question,\n",
    "    option_a=options[0],\n",
    "    option_b=options[1],\n",
    "    option_c=options[2],\n",
    "    option_d=options[3],\n",
    ")\n",
    "\n",
    "generations = generator(\n",
    "    sample_prompt_formatted, max_new_tokens=30, num_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, generation in enumerate(generations):\n",
    "    print(\n",
    "        f\"Attempt {i+1}:\", generation[\"generated_text\"][len(sample_prompt_formatted) :]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N-17qxQNwLH"
   },
   "source": [
    "As you can see, if we tried to run it automatically in the background, it would be rather a mess!\n",
    "\n",
    "We start with a simple implementation where we will also utilise [caching](https://huggingface.co/docs/transformers/en/kv_cache) to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ -9.76305008  -9.7662611   -8.91489983 -10.23245716]\n",
      "Is correct: True\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def compute_unnormalised_log_prob_sequentially(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prompt: str,\n",
    "    completions: List[str],\n",
    "    correct: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sequentially computes log probabilities of completions using KV caching.\n",
    "    \"\"\"\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate KV cache for question - shared part of each completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **prompt_inputs,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        prompt_kv_cache = outputs.past_key_values\n",
    "\n",
    "    log_probs_list = []\n",
    "\n",
    "    # Process all completions sequentially\n",
    "    for completion in completions:\n",
    "        # Tokenize only the completion\n",
    "        completion_inputs = tokenizer(completion, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Run the model with the cached KV from the prompt\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=completion_inputs.input_ids,\n",
    "                past_key_values=prompt_kv_cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Get log probs of the actual next tokens\n",
    "        token_log_probs = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            completion_inputs.input_ids[None, ...],\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Sum the log probs to get the sequence log prob\n",
    "        seq_log_prob = token_log_probs.sum()\n",
    "        log_probs_list.append(seq_log_prob.item())\n",
    "\n",
    "    log_probs_list = np.array(log_probs_list)\n",
    "    is_correct = np.argmax(log_probs_list) == ord(\"D\") - ord(correct)\n",
    "    return log_probs_list, is_correct\n",
    "\n",
    "\n",
    "scores_sequential, is_correct = compute_unnormalised_log_prob_sequentially(\n",
    "    model, tokenizer, sample_prompt_formatted, options, answer\n",
    ")\n",
    "\n",
    "print(\"Scores:\", scores_sequential)\n",
    "print(\"Is correct:\", is_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb2omvRBNwLH"
   },
   "source": [
    "##### TASK decoder vectorized:\n",
    "\n",
    "Now your task is to implement vectorized version of this code. We don't want to make forward passes through the model with batch size = 1 in a for loop, that is very inefficient. We want to make forward passes with batch size = number of options (4 in that case).\n",
    "\n",
    "The perplexity calculation after the forward pass doesn't need to be vectorized.\n",
    "\n",
    "    1. Create KV cache with past key values for the shared prompt part - question (2 pkt)\n",
    "    2. Repeat KV cache to make the shapes right for batched options (2 pkt)\n",
    "    3. Calculate perplexity for each option. Make sure not to include padding tokens! (2 pkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unnormalised_log_prob_sequentially(\n",
    "    model, tokenizer, prompt: str, completions: list, correct: str\n",
    "):\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_inputs, use_cache=True)\n",
    "        prompt_kv_cache = outputs.past_key_values\n",
    "\n",
    "    log_probs_list = []\n",
    "\n",
    "    for completion in completions:\n",
    "        completion_inputs = tokenizer(completion, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=completion_inputs.input_ids,\n",
    "                past_key_values=prompt_kv_cache,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        logits = outputs.logits\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        input_ids = completion_inputs.input_ids[0]\n",
    "        seq_log_prob = sum([\n",
    "            log_probs[0, i, token_id]\n",
    "            for i, token_id in enumerate(input_ids)\n",
    "        ])\n",
    "        log_probs_list.append(seq_log_prob)\n",
    "\n",
    "    log_probs_list = np.array(log_probs_list)\n",
    "    correct_index = ord(correct.upper()) - ord(\"A\")\n",
    "    is_correct = np.argmax(log_probs_list) == correct_index\n",
    "    return log_probs_list, is_correct\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unnormalised_log_prob_vectorized(\n",
    "    model, tokenizer, prompt: str, completions: list, correct: str\n",
    "):\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**prompt_inputs, use_cache=True)\n",
    "        prompt_kv_cache = outputs.past_key_values\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    completion_inputs = tokenizer(completions, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    input_ids = completion_inputs.input_ids\n",
    "    attention_mask = completion_inputs.attention_mask\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    batched_prompt_kv_cache = tuple([\n",
    "        (\n",
    "            k.expand(batch_size, *k.shape[1:]).contiguous(),\n",
    "            v.expand(batch_size, *v.shape[1:]).contiguous()\n",
    "        ) for (k, v) in prompt_kv_cache\n",
    "    ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=batched_prompt_kv_cache,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    log_probs_list = []\n",
    "    for i in range(batch_size):\n",
    "        seq_log_prob = 0.0\n",
    "        for t in range(input_ids.shape[1]):\n",
    "            token_id = input_ids[i, t]\n",
    "            if token_id == tokenizer.pad_token_id:\n",
    "                continue\n",
    "            seq_log_prob += log_probs[i, t, token_id]\n",
    "        log_probs_list.append(seq_log_prob)\n",
    "\n",
    "    log_probs_list = np.array(log_probs_list)\n",
    "    correct_index = ord(correct.upper()) - ord(\"A\")\n",
    "    is_correct = np.argmax(log_probs_list) == correct_index\n",
    "    return log_probs_list, is_correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ -9.76305   -9.766261  -8.9149   -10.232457]\n",
      "Is correct: False\n"
     ]
    }
   ],
   "source": [
    "scores_sequential, _ = compute_unnormalised_log_prob_sequentially(\n",
    "    model, tokenizer, sample_prompt_formatted, options, answer\n",
    ")\n",
    "\n",
    "scores_vectorized, is_correct = compute_unnormalised_log_prob_vectorized(\n",
    "    model, tokenizer, sample_prompt_formatted, options, answer\n",
    ")\n",
    "print(\"Scores:\", scores_sequential)\n",
    "print(\"Is correct:\", is_correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "hQZI4HWhNwLN"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(scores_sequential, scores_vectorized), \"Mismatch between sequential and vectorized scores!\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
